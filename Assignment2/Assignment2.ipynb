{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Задача 1.\n",
        "\n",
        "**Гетерогенная параллелизация** — это подход к параллельным вычислениям, при котором в рамках одной программы или системы совместно используются **разные типы вычислительных устройств**, чаще всего **CPU и GPU**, а иногда также FPGA, TPU и другие ускорители.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Различия между параллельными вычислениями на CPU и GPU\n",
        "\n",
        "**CPU (Central Processing Unit):**\n",
        "\n",
        "* Имеет небольшое количество мощных ядер (обычно от 4 до 32).\n",
        "* Оптимизирован для последовательных и слабо параллельных задач.\n",
        "* Обладает сложной логикой управления, большим кэшем, эффективен при ветвлениях и условных операциях.\n",
        "* Используется для управления программой, выполнения логики, ввода-вывода.\n",
        "\n",
        "**GPU (Graphics Processing Unit):**\n",
        "\n",
        "* Содержит тысячи более простых вычислительных ядер.\n",
        "* Оптимизирован для массового параллелизма и однотипных операций над большими массивами данных.\n",
        "* Наиболее эффективен для задач с высокой степенью параллелизма и минимальным ветвлением.\n",
        "* Используется для вычислений над векторами, матрицами, изображениями, нейронными сетями.\n",
        "\n",
        "CPU лучше подходит для сложной логики и управления, GPU — для интенсивных численных вычислений с высокой степенью параллелизма.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Преимущества гетерогенной параллелизации\n",
        "\n",
        "* **Повышение производительности** за счёт распределения задач между CPU и GPU.\n",
        "* **Энергоэффективность**: GPU выполняет массовые вычисления быстрее и с меньшими затратами энергии.\n",
        "* **Гибкость**: разные части программы могут быть оптимизированы под разные архитектуры.\n",
        "* **Масштабируемость**: возможность подключения дополнительных ускорителей.\n",
        "* **Оптимальное использование ресурсов** системы.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Примеры реальных приложений гетерогенной параллелизации\n",
        "\n",
        "* **Машинное обучение и искусственный интеллект**\n",
        "  Обучение нейронных сетей (TensorFlow, PyTorch): CPU управляет процессом, GPU выполняет матричные операции.\n",
        "\n",
        "* **Обработка изображений и видео**\n",
        "  Кодирование видео, фильтрация изображений, компьютерное зрение (OpenCV с CUDA).\n",
        "\n",
        "* **Научные и инженерные расчёты**\n",
        "  Моделирование климата, молекулярная динамика, численные методы в физике и химии.\n",
        "\n",
        "* **Игровые движки**\n",
        "  CPU отвечает за логику игры и физику, GPU — за рендеринг графики и шейдеры.\n",
        "\n",
        "* **Финансовые вычисления**\n",
        "  Анализ больших массивов данных, моделирование рисков, высокочастотный трейдинг.\n",
        "\n",
        "---\n",
        "\n",
        "### В итоге\n",
        "\n",
        "Гетерогенная параллелизация позволяет эффективно сочетать сильные стороны CPU и GPU, обеспечивая высокую производительность и оптимальное использование вычислительных ресурсов. Этот подход является ключевым в современных высокопроизводительных и ресурсоёмких приложениях.\n"
      ],
      "metadata": {
        "id": "PmPEt8kGRhKR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubcSu0u3RH9W",
        "outputId": "c99a4cca-b68e-4b2c-f7ff-7be82ffed931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing task2.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile task2.cpp\n",
        "\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cstdlib>\n",
        "#include <ctime>\n",
        "#include <omp.h>\n",
        "\n",
        "int main() {\n",
        "    const int N = 10000;              // Размер массива\n",
        "    std::vector<int> arr(N);          // Динамический массив на N элементов\n",
        "\n",
        "    // Инициализация генератора случайных чисел\n",
        "    srand(time(0));\n",
        "\n",
        "    // Заполняем массив случайными числами\n",
        "    for (int i = 0; i < N; i++)\n",
        "        arr[i] = rand();\n",
        "\n",
        "    // ------------------- Последовательная версия -------------------\n",
        "\n",
        "    // Засекаем время начала\n",
        "    double start = omp_get_wtime();\n",
        "\n",
        "    // Изначально считаем, что минимум и максимум — первый элемент\n",
        "    int minVal = arr[0], maxVal = arr[0];\n",
        "\n",
        "    // Обычный проход по массиву\n",
        "    for (int i = 1; i < N; i++) {\n",
        "        if (arr[i] < minVal) minVal = arr[i];   // Обновляем минимум\n",
        "        if (arr[i] > maxVal) maxVal = arr[i];   // Обновляем максимум\n",
        "    }\n",
        "\n",
        "    // Засекаем время окончания\n",
        "    double end = omp_get_wtime();\n",
        "\n",
        "    // Вывод результата и времени работы\n",
        "    std::cout << \"Sequential min: \" << minVal\n",
        "              << \" max: \" << maxVal\n",
        "              << \" time: \" << end - start << std::endl;\n",
        "\n",
        "    // ------------------- Параллельная версия -------------------\n",
        "\n",
        "    // Снова засекаем время\n",
        "    start = omp_get_wtime();\n",
        "\n",
        "    // Сбрасываем минимум и максимум\n",
        "    minVal = arr[0];\n",
        "    maxVal = arr[0];\n",
        "\n",
        "    // Параллельный цикл\n",
        "    // В конце OpenMP объединяет результаты\n",
        "    #pragma omp parallel for reduction(min:minVal) reduction(max:maxVal)\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        if (arr[i] < minVal) minVal = arr[i];\n",
        "        if (arr[i] > maxVal) maxVal = arr[i];\n",
        "    }\n",
        "\n",
        "    // Конец замера времени\n",
        "    end = omp_get_wtime();\n",
        "\n",
        "    // Вывод результата и времени работы параллельной версии\n",
        "    std::cout << \"Parallel min: \" << minVal\n",
        "              << \" max: \" << maxVal\n",
        "              << \" time: \" << end - start << std::endl;\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! g++ -fopenmp -o2 task2.cpp -o anyname\n",
        "!./anyname"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IYehXqtSPZV",
        "outputId": "c913837b-f80f-4292-84d4-1483c3e82d82"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential min: 17029 max: 2147205612 time: 7.033e-05\n",
            "Parallel min: 17029 max: 2147205612 time: 0.00184204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task3.cpp\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cstdlib>\n",
        "#include <ctime>\n",
        "#include <omp.h>\n",
        "#include <algorithm>\n",
        "\n",
        "// Заполнение массива случайными числами\n",
        "void fillArray(std::vector<int>& a) {\n",
        "    for (int& x : a)\n",
        "        x = rand() % 100000;\n",
        "}\n",
        "\n",
        "// Последовательная сортировка выбором\n",
        "void selectionSortSequential(std::vector<int>& a) {\n",
        "    int n = a.size();\n",
        "\n",
        "    for (int i = 0; i < n - 1; i++) {\n",
        "        int minIndex = i;\n",
        "\n",
        "        // Поиск минимального элемента\n",
        "        for (int j = i + 1; j < n; j++) {\n",
        "            if (a[j] < a[minIndex])\n",
        "                minIndex = j;\n",
        "        }\n",
        "\n",
        "        // Перестановка элементов\n",
        "        std::swap(a[i], a[minIndex]);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Параллельная сортировка выбором с OpenMP\n",
        "void selectionSortParallel(std::vector<int>& a) {\n",
        "    int n = a.size();\n",
        "\n",
        "    for (int i = 0; i < n - 1; i++) {\n",
        "        int minIndex = i;\n",
        "\n",
        "        // Параллельный поиск минимального элемента\n",
        "        #pragma omp parallel for\n",
        "        for (int j = i + 1; j < n; j++) {\n",
        "\n",
        "            // Защита общей переменной minIndex\n",
        "            #pragma omp critical\n",
        "            {\n",
        "                if (a[j] < a[minIndex])\n",
        "                    minIndex = j;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Обмен найденного минимума\n",
        "        std::swap(a[i], a[minIndex]);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    srand(time(nullptr));\n",
        "\n",
        "    const int N1 = 1000;\n",
        "    const int N2 = 10000;\n",
        "\n",
        "    // ====== Тест для массива из 1000 элементов ======\n",
        "    std::vector<int> arr1(N1);\n",
        "    fillArray(arr1);\n",
        "\n",
        "    std::vector<int> arr1Copy = arr1;\n",
        "\n",
        "    double start = omp_get_wtime();\n",
        "    selectionSortSequential(arr1);\n",
        "    double end = omp_get_wtime();\n",
        "    std::cout << \"Sequential sort (1000): \" << end - start << \" sec\\n\";\n",
        "\n",
        "    start = omp_get_wtime();\n",
        "    selectionSortParallel(arr1Copy);\n",
        "    end = omp_get_wtime();\n",
        "    std::cout << \"Parallel sort (1000): \" << end - start << \" sec\\n\\n\";\n",
        "\n",
        "    // ====== Тест для массива из 10000 элементов ======\n",
        "    std::vector<int> arr2(N2);\n",
        "    fillArray(arr2);\n",
        "\n",
        "    std::vector<int> arr2Copy = arr2;\n",
        "\n",
        "    start = omp_get_wtime();\n",
        "    selectionSortSequential(arr2);\n",
        "    end = omp_get_wtime();\n",
        "    std::cout << \"Sequential sort (10000): \" << end - start << \" sec\\n\";\n",
        "\n",
        "    start = omp_get_wtime();\n",
        "    selectionSortParallel(arr2Copy);\n",
        "    end = omp_get_wtime();\n",
        "    std::cout << \"Parallel sort (10000): \" << end - start << \" sec\\n\";\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-ARxK2ISP6s",
        "outputId": "886fb4ef-b8cb-4369-b5cd-07154a0e19e5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing task3.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! g++ -fopenmp -o2 task3.cpp -o anyname\n",
        "!./anyname"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV4hu1EFSQOs",
        "outputId": "4fac09e3-a531-4d99-ddad-4f1fe826fd6e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential sort (1000): 0.00210048 sec\n",
            "Parallel sort (1000): 0.0149562 sec\n",
            "\n",
            "Sequential sort (10000): 0.319791 sec\n",
            "Parallel sort (10000): 2.9411 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task4.cpp\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cstdlib>\n",
        "#include <ctime>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// Заполнение массива случайными числами\n",
        "void fillArray(std::vector<int>& a) {\n",
        "    for (int& x : a)\n",
        "        x = rand() % 100000;\n",
        "}\n",
        "__global__ void mergeKernel(int* input, int* output, int width, int n) {\n",
        "\n",
        "    // Глобальный индекс потока\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Левая граница подмассива\n",
        "    int left = idx * 2 * width;\n",
        "\n",
        "    if (left < n) {\n",
        "        int mid = min(left + width, n);        // середина\n",
        "        int right = min(left + 2 * width, n);  // правая граница\n",
        "\n",
        "        int i = left;\n",
        "        int j = mid;\n",
        "        int k = left;\n",
        "\n",
        "        // Слияние двух отсортированных подмассивов\n",
        "        while (i < mid && j < right) {\n",
        "            if (input[i] <= input[j])\n",
        "                output[k++] = input[i++];\n",
        "            else\n",
        "                output[k++] = input[j++];\n",
        "        }\n",
        "\n",
        "        // Копирование оставшихся элементов\n",
        "        while (i < mid)\n",
        "            output[k++] = input[i++];\n",
        "        while (j < right)\n",
        "            output[k++] = input[j++];\n",
        "    }\n",
        "}\n",
        "\n",
        "void mergeSortGPU(std::vector<int>& data) {\n",
        "    int n = data.size();\n",
        "    int* d_input;\n",
        "    int* d_output;\n",
        "\n",
        "    size_t size = n * sizeof(int);\n",
        "\n",
        "    // Выделение памяти на GPU\n",
        "    cudaMalloc(&d_input, size);\n",
        "    cudaMalloc(&d_output, size);\n",
        "\n",
        "    // Копирование данных на GPU\n",
        "    cudaMemcpy(d_input, data.data(), size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocks;\n",
        "\n",
        "    // Итеративное увеличение размера подмассивов\n",
        "    for (int width = 1; width < n; width *= 2) {\n",
        "        blocks = (n + (2 * width * threadsPerBlock) - 1) / (2 * width * threadsPerBlock);\n",
        "\n",
        "        mergeKernel<<<blocks, threadsPerBlock>>>(d_input, d_output, width, n);\n",
        "\n",
        "        // Обмен указателей\n",
        "        std::swap(d_input, d_output);\n",
        "    }\n",
        "\n",
        "    // Копирование результата обратно на CPU\n",
        "    cudaMemcpy(data.data(), d_input, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Освобождение памяти\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    srand(time(nullptr));\n",
        "\n",
        "    const int N1 = 10000;\n",
        "    const int N2 = 100000;\n",
        "\n",
        "    std::vector<int> arr1(N1);\n",
        "    fillArray(arr1);\n",
        "\n",
        "    std::vector<int> arr2(N2);\n",
        "    fillArray(arr2);\n",
        "\n",
        "    // ===== Массив 10 000 =====\n",
        "    clock_t start = clock();\n",
        "    mergeSortGPU(arr1);\n",
        "    clock_t end = clock();\n",
        "\n",
        "    std::cout << \"GPU merge sort (10000): \"\n",
        "              << double(end - start) / CLOCKS_PER_SEC\n",
        "              << \" sec\\n\";\n",
        "\n",
        "    // ===== Массив 100 000 =====\n",
        "    start = clock();\n",
        "    mergeSortGPU(arr2);\n",
        "    end = clock();\n",
        "\n",
        "    std::cout << \"GPU merge sort (100000): \"\n",
        "              << double(end - start) / CLOCKS_PER_SEC\n",
        "              << \" sec\\n\";\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7en7laTZSSbk",
        "outputId": "4eb5265e-f93c-47ee-a85a-5156021e67b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing task4.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! g++ -fopenmp -o2 task4.cpp -o anyname\n",
        "!./anyname"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awIWx9K1SSx-",
        "outputId": "4166f033-2913-4678-e555-a0bd4e50e692"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Ktask4.cpp:5:10:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kcuda_runtime.h: No such file or directory\n",
            "    5 | #include \u001b[01;31m\u001b[K<cuda_runtime.h>\u001b[m\u001b[K\n",
            "      |          \u001b[01;31m\u001b[K^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "compilation terminated.\n",
            "Sequential sort (1000): 0.00224574 sec\n",
            "Parallel sort (1000): 0.0168591 sec\n",
            "\n",
            "Sequential sort (10000): 0.208004 sec\n",
            "Parallel sort (10000): 1.48886 sec\n"
          ]
        }
      ]
    }
  ]
}