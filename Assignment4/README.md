# Assignment 4

**1. В чём заключается отличие гибридных вычислений от вычислений только на CPU или только на GPU?**

Гибридные вычисления используют одновременно CPU и GPU, распределяя работу между ними. В отличие от чисто CPU- или GPU-реализаций, гибридный подход позволяет задействовать сильные стороны обоих устройств: CPU для управления, ветвлений и последовательных частей, GPU для массово-параллельных вычислений.

---

**2. Для каких типов задач целесообразно распределять вычисления между CPU и GPU?**

Такой подход эффективен для задач, где часть алгоритма хорошо параллелится, а часть требует сложной логики или последовательной обработки. Примеры: обработка больших массивов данных, численное моделирование, анализ сигналов, машинное обучение, когда подготовка данных выполняется на CPU, а вычислительно тяжёлая часть — на GPU.

---

**3. В чём разница между синхронной и асинхронной передачей данных между CPU и GPU?**

При синхронной передаче данных CPU ожидает завершения копирования, и выполнение программы блокируется. Асинхронная передача позволяет запускать копирование и вычисления параллельно, не останавливая работу CPU или GPU.

---

**4. Почему асинхронная передача данных может повысить производительность программы?**

Асинхронная передача позволяет перекрывать вычисления и обмен данными. Пока GPU выполняет ядро, CPU может готовить новые данные или выполнять другие задачи, что снижает простои и улучшает использование ресурсов.

---

**5. Какие основные функции MPI используются для распределения и сбора данных между процессами?**

Основными функциями являются `MPI_Send` и `MPI_Recv` для точечного обмена, а также коллективные операции `MPI_Bcast`, `MPI_Scatter`, `MPI_Gather` и `MPI_Reduce`, которые упрощают распределение и объединение данных между процессами.

---

**6. Как количество процессов MPI влияет на время выполнения программы и почему?**

Увеличение числа процессов может сократить время выполнения за счёт параллелизма, однако после определённого момента эффективность снижается. Это связано с ростом накладных расходов на коммуникацию и синхронизацию между процессами.

---

**7. Какие факторы ограничивают масштабируемость распределённых параллельных программ?**

Основными ограничениями являются задержки сети, пропускная способность каналов связи, неравномерная нагрузка между процессами, синхронизации и доля последовательного кода в программе.

---

**8. В каких случаях использование распределённых вычислений оправдано, а в каких — неэффективно?**

Распределённые вычисления оправданы при обработке больших объёмов данных и длительных вычислениях, которые невозможно выполнить на одной машине за разумное время. Они неэффективны для небольших задач, где накладные расходы на обмен данными и управление превышают выигрыш от параллелизма.
