{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Задание 1"
      ],
      "metadata": {
        "id": "lNeAqcE53pHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Последовательная реализация на CPU"
      ],
      "metadata": {
        "id": "7uh4Jyw63wxK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA6ns9QC2yqF",
        "outputId": "47d45e30-3717-4571-e1cc-b542a5f51215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing consecutive.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile consecutive.cpp\n",
        "\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <chrono>\n",
        "\n",
        "int main() {\n",
        "    // Размер массива\n",
        "    const int N = 100000;\n",
        "\n",
        "    // Инициализация массива значениями 1\n",
        "    std::vector<int> a(N, 1);\n",
        "\n",
        "    // Фиксация времени начала вычислений\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Последовательное суммирование элементов массива\n",
        "    long long cpu_sum = 0;\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        cpu_sum += a[i];\n",
        "    }\n",
        "\n",
        "    // Фиксация времени окончания вычислений\n",
        "    auto end = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Вычисление затраченного времени в миллисекундах\n",
        "    std::chrono::duration<double, std::milli> cpu_time = end - start;\n",
        "\n",
        "    // Вывод результата и времени выполнения\n",
        "    std::cout << \"CPU sum: \" << cpu_sum << std::endl;\n",
        "    std::cout << \"CPU time: \" << cpu_time.count() << \" ms\" << std::endl;\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ consecutive.cpp -o consecutive\n",
        "!./consecutive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXIBI_n03-V6",
        "outputId": "5119bcf0-3659-4641-de5e-c1276a02950e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU sum: 100000\n",
            "CPU time: 0.325889 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CUDA-реализация с использованием глобальной памяти"
      ],
      "metadata": {
        "id": "6TRdIzp93xFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cuda.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// CUDA-ядро для суммирования элементов массива\n",
        "__global__ void sumKernel(int* d_a, int* d_sum, int n) {\n",
        "    // Глобальный индекс потока\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Проверка выхода за границы массива\n",
        "    if (idx < n) {\n",
        "        // Атомарное добавление элемента массива к общей сумме\n",
        "        // Используется глобальная память\n",
        "        atomicAdd(d_sum, d_a[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Размер массива\n",
        "    const int N = 100000;\n",
        "    const int size = N * sizeof(int);\n",
        "\n",
        "    // Выделение и инициализация массива на host\n",
        "    int* h_a = new int[N];\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        h_a[i] = 1;\n",
        "    }\n",
        "\n",
        "    // Указатели на память device\n",
        "    int* d_a;\n",
        "    int* d_sum;\n",
        "\n",
        "    // Выделение памяти на GPU\n",
        "    cudaMalloc(&d_a, size);\n",
        "    cudaMalloc(&d_sum, sizeof(int));\n",
        "\n",
        "    // Копирование данных с host на device\n",
        "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Обнуление суммы на device\n",
        "    cudaMemset(d_sum, 0, sizeof(int));\n",
        "\n",
        "    // Конфигурация запуска ядра\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocks = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "\n",
        "    // Создание CUDA-событий для замера времени\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // Запуск таймера и CUDA-ядра\n",
        "    cudaEventRecord(start);\n",
        "    sumKernel<<<blocks, threadsPerBlock>>>(d_a, d_sum, N);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    // Получение времени выполнения ядра\n",
        "    float gpu_time = 0.0f;\n",
        "    cudaEventElapsedTime(&gpu_time, start, stop);\n",
        "\n",
        "    // Копирование результата обратно на host\n",
        "    int gpu_sum;\n",
        "    cudaMemcpy(&gpu_sum, d_sum, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Вывод результата и времени выполнения\n",
        "    std::cout << \"GPU sum: \" << gpu_sum << std::endl;\n",
        "    std::cout << \"GPU time: \" << gpu_time << \" ms\" << std::endl;\n",
        "\n",
        "    // Освобождение памяти\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_sum);\n",
        "    delete[] h_a;\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhr72Qr34BKY",
        "outputId": "478a5cda-55fd-4a6f-8a96-7b25d8864526"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cuda.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc cuda.cu -arch=compute_75 -code=sm_75 -o cuda\n",
        "!./cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBE6j1gG4BZO",
        "outputId": "7ac7b676-1bde-4fa3-dc96-4b725aed1c64"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU sum: 100000\n",
            "GPU time: 0.088096 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В обоих случаях получена одинаковая сумма элементов массива, равная 100000, что подтверждает корректность реализации как последовательного алгоритма на CPU, так и CUDA-версии на GPU.\n",
        "\n",
        "Время выполнения на GPU оказалось меньше, чем на CPU. Это связано с тем, что вычисление суммы выполняется параллельно большим количеством потоков, тогда как CPU-реализация использует один поток. Несмотря на использование атомарных операций в глобальной памяти, которые снижают эффективность параллелизма, GPU показывает более высокую производительность для данной задачи.\n",
        "\n",
        "Следует отметить, что измеряемое время на GPU включает только время выполнения CUDA-ядра и не учитывает накладные расходы на выделение памяти и копирование данных между host и device. При их учёте преимущество GPU для массива такого размера может уменьшиться."
      ],
      "metadata": {
        "id": "lalTgf4A5SsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 2"
      ],
      "metadata": {
        "id": "sV5Pku_S5Ug_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Последовательная реализация на CPU"
      ],
      "metadata": {
        "id": "5seAOhgq9Auz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile consecutive_scan.cpp\n",
        "\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <chrono>\n",
        "\n",
        "int main() {\n",
        "    // Размер массива\n",
        "    const int N = 1'000'000;\n",
        "\n",
        "    // Входной массив и массив префиксных сумм\n",
        "    std::vector<int> a(N, 1);\n",
        "    std::vector<int> prefix(N);\n",
        "\n",
        "    // Фиксация времени начала вычислений\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Последовательное вычисление префиксной суммы\n",
        "    prefix[0] = a[0];\n",
        "    for (int i = 1; i < N; i++) {\n",
        "        prefix[i] = prefix[i - 1] + a[i];\n",
        "    }\n",
        "\n",
        "    // Фиксация времени окончания вычислений\n",
        "    auto end = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Вычисление времени выполнения в миллисекундах\n",
        "    std::chrono::duration<double, std::milli> cpu_time = end - start;\n",
        "\n",
        "    // Вывод последнего элемента (проверка корректности)\n",
        "    std::cout << \"CPU last element: \" << prefix[N - 1] << std::endl;\n",
        "    std::cout << \"CPU time: \" << cpu_time.count() << \" ms\" << std::endl;\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvshez3c5VJW",
        "outputId": "a5fe7b31-1adf-42e4-82b5-3b64b75a02ea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing consecutive_scan.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ consecutive_scan.cpp -o consecutive_scan\n",
        "!./consecutive_scan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WORIIl1w9C-x",
        "outputId": "3f4fe040-f608-4aa4-aecd-b1d25eef2d19"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU last element: 1000000\n",
            "CPU time: 7.72496 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CUDA-реализация с shared memory"
      ],
      "metadata": {
        "id": "KLfoy7bW9DNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cuda_scan.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "/*\n",
        " * Kernel выполняет префиксную сумму внутри одного блока.\n",
        " * Для ускорения используется разделяемая память (shared memory).\n",
        " * Также сохраняется сумма элементов каждого блока.\n",
        " */\n",
        "__global__ void scanKernel(int* d_in, int* d_out, int* block_sums, int n) {\n",
        "    // Разделяемая память для текущего блока\n",
        "    extern __shared__ int temp[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int gid = blockIdx.x * blockDim.x + tid;\n",
        "\n",
        "    // Загрузка данных из глобальной памяти в shared memory\n",
        "    if (gid < n)\n",
        "        temp[tid] = d_in[gid];\n",
        "    else\n",
        "        temp[tid] = 0;\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // Параллельный scan (алгоритм Hillis–Steele)\n",
        "    for (int offset = 1; offset < blockDim.x; offset <<= 1) {\n",
        "        int value = 0;\n",
        "        if (tid >= offset)\n",
        "            value = temp[tid - offset];\n",
        "\n",
        "        __syncthreads();\n",
        "        temp[tid] += value;\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Запись локального результата обратно в глобальную память\n",
        "    if (gid < n)\n",
        "        d_out[gid] = temp[tid];\n",
        "\n",
        "    // Последний поток блока сохраняет сумму элементов блока\n",
        "    if (tid == blockDim.x - 1)\n",
        "        block_sums[blockIdx.x] = temp[tid];\n",
        "}\n",
        "\n",
        "/*\n",
        " * Kernel добавляет к каждому элементу сумму всех предыдущих блоков\n",
        " */\n",
        "__global__ void addBlockSums(int* d_out, int* block_prefix, int n) {\n",
        "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (blockIdx.x > 0 && gid < n) {\n",
        "        d_out[gid] += block_prefix[blockIdx.x - 1];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Размер массива\n",
        "    const int N = 1'000'000;\n",
        "    const int size = N * sizeof(int);\n",
        "\n",
        "    // Параметры запуска CUDA\n",
        "    const int threadsPerBlock = 256;\n",
        "    const int blocks = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "\n",
        "    // Выделение и инициализация входного массива на host\n",
        "    int* h_in = new int[N];\n",
        "    for (int i = 0; i < N; i++)\n",
        "        h_in[i] = 1;\n",
        "\n",
        "    // Указатели на память device\n",
        "    int *d_in, *d_out, *d_block_sums;\n",
        "\n",
        "    cudaMalloc(&d_in, size);\n",
        "    cudaMalloc(&d_out, size);\n",
        "    cudaMalloc(&d_block_sums, blocks * sizeof(int));\n",
        "\n",
        "    // Копирование данных с host на device\n",
        "    cudaMemcpy(d_in, h_in, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // CUDA-события для замера времени выполнения\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    // 1. Вычисление префиксной суммы внутри каждого блока\n",
        "    scanKernel<<<blocks, threadsPerBlock, threadsPerBlock * sizeof(int)>>>(\n",
        "        d_in, d_out, d_block_sums, N);\n",
        "\n",
        "    // 2. Копирование сумм блоков на host\n",
        "    int* h_block_sums = new int[blocks];\n",
        "    cudaMemcpy(h_block_sums, d_block_sums,\n",
        "               blocks * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Последовательное вычисление префиксной суммы блоков\n",
        "    for (int i = 1; i < blocks; i++)\n",
        "        h_block_sums[i] += h_block_sums[i - 1];\n",
        "\n",
        "    // Копирование результатов обратно на device\n",
        "    cudaMemcpy(d_block_sums, h_block_sums,\n",
        "               blocks * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // 3. Добавление сумм предыдущих блоков\n",
        "    addBlockSums<<<blocks, threadsPerBlock>>>(d_out, d_block_sums, N);\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    // Получение времени выполнения GPU-версии\n",
        "    float gpu_time = 0.0f;\n",
        "    cudaEventElapsedTime(&gpu_time, start, stop);\n",
        "\n",
        "    // Копирование последнего элемента для проверки корректности\n",
        "    int last;\n",
        "    cudaMemcpy(&last, &d_out[N - 1], sizeof(int),\n",
        "               cudaMemcpyDeviceToHost);\n",
        "\n",
        "    std::cout << \"GPU last element: \" << last << std::endl;\n",
        "    std::cout << \"GPU time: \" << gpu_time << \" ms\" << std::endl;\n",
        "\n",
        "    // Освобождение памяти\n",
        "    cudaFree(d_in);\n",
        "    cudaFree(d_out);\n",
        "    cudaFree(d_block_sums);\n",
        "    delete[] h_in;\n",
        "    delete[] h_block_sums;\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DTV0mR_9FJT",
        "outputId": "afe35092-95cc-43f8-fee9-c1635437c11d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cuda_scan.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc cuda_scan.cu -arch=compute_75 -code=sm_75 -o cuda_scan\n",
        "!./cuda_scan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNVUQAeb9Fd3",
        "outputId": "d19147c5-00bd-4957-c478-1693ec7f6101"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU last element: 1000000\n",
            "GPU time: 0.33136 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обе реализации корректно вычисляют префиксную сумму, что подтверждается совпадением результатов. Последовательная реализация на CPU выполнилась за 7.72 мс, тогда как CUDA-реализация с использованием разделяемой памяти показала время 0.33 мс, обеспечив ускорение более чем в 20 раз. Полученный выигрыш объясняется эффективным распараллеливанием вычислений и снижением числа обращений к глобальной памяти за счёт использования shared memory."
      ],
      "metadata": {
        "id": "tiHG04rx9yj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 3"
      ],
      "metadata": {
        "id": "rCLB5Zne9y9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Последовательная реализация на CPU"
      ],
      "metadata": {
        "id": "HSSASGlhCjDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile sequential3.cpp\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <chrono>\n",
        "\n",
        "// Функция последовательного суммирования элементов массива на CPU\n",
        "long long cpu_sum(const std::vector<int>& a) {\n",
        "    long long sum = 0;\n",
        "    for (int x : a)\n",
        "        sum += x;\n",
        "    return sum;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Размер массива\n",
        "    const int N = 1'000'000;\n",
        "\n",
        "    // Инициализация массива значениями 1\n",
        "    std::vector<int> a(N, 1);\n",
        "\n",
        "    // Фиксация времени начала вычислений\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Последовательное вычисление суммы на CPU\n",
        "    long long result = cpu_sum(a);\n",
        "\n",
        "    // Фиксация времени окончания вычислений\n",
        "    auto end = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Вычисление времени выполнения\n",
        "    std::chrono::duration<double, std::milli> time = end - start;\n",
        "\n",
        "    // Вывод результата и времени\n",
        "    std::cout << \"CPU sum: \" << result << std::endl;\n",
        "    std::cout << \"CPU time: \" << time.count() << \" ms\" << std::endl;\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6s5rJi7CdaT",
        "outputId": "6ae348c9-b88d-40c2-f051-d3389c208629"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing sequential3.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ sequential3.cpp -o sequential3\n",
        "!./sequential3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GELMrzOvDFWB",
        "outputId": "a022dd97-2d49-464c-824c-be26162c004c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU sum: 1000000\n",
            "CPU time: 17.2313 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализация на GPU (глобальная память + atomicAdd)"
      ],
      "metadata": {
        "id": "DpxhRk7YCwe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gpu3.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// CUDA-ядро для суммирования элементов массива\n",
        "// Каждый поток обрабатывает один элемент\n",
        "__global__ void sumKernel(int* d_a, int* d_sum, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Проверка выхода за границы массива\n",
        "    if (idx < n) {\n",
        "        // Атомарное добавление значения к общей сумме\n",
        "        atomicAdd(d_sum, d_a[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Размер массива\n",
        "    const int N = 1'000'000;\n",
        "    const int size = N * sizeof(int);\n",
        "\n",
        "    // Выделение и инициализация массива на host\n",
        "    int* h_a = new int[N];\n",
        "    for (int i = 0; i < N; i++)\n",
        "        h_a[i] = 1;\n",
        "\n",
        "    // Указатели на память device\n",
        "    int *d_a, *d_sum;\n",
        "\n",
        "    // Выделение памяти на GPU\n",
        "    cudaMalloc(&d_a, size);\n",
        "    cudaMalloc(&d_sum, sizeof(int));\n",
        "\n",
        "    // Копирование данных с host на device\n",
        "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Инициализация суммы нулём\n",
        "    cudaMemset(d_sum, 0, sizeof(int));\n",
        "\n",
        "    // Параметры запуска CUDA-ядра\n",
        "    int threads = 256;\n",
        "    int blocks = (N + threads - 1) / threads;\n",
        "\n",
        "    // CUDA-события для замера времени выполнения\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // Запуск таймера и CUDA-ядра\n",
        "    cudaEventRecord(start);\n",
        "    sumKernel<<<blocks, threads>>>(d_a, d_sum, N);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    // Получение времени выполнения ядра\n",
        "    float gpu_time;\n",
        "    cudaEventElapsedTime(&gpu_time, start, stop);\n",
        "\n",
        "    // Копирование результата обратно на host\n",
        "    int result;\n",
        "    cudaMemcpy(&result, d_sum, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Вывод результата и времени\n",
        "    std::cout << \"GPU sum: \" << result << std::endl;\n",
        "    std::cout << \"GPU time: \" << gpu_time << \" ms\" << std::endl;\n",
        "\n",
        "    // Освобождение памяти\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_sum);\n",
        "    delete[] h_a;\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7r3AF7WlCdPJ",
        "outputId": "dee6e26f-ae56-44ca-f59f-3f53b856bb4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gpu3.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc gpu3.cu -arch=compute_75 -code=sm_75 -o gpu3\n",
        "!./gpu3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpUdxxOWDC6D",
        "outputId": "cb56aac4-82a8-42ba-9433-a8fc9ea115b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU sum: 1000000\n",
            "GPU time: 0.155456 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Гибридная реализация CPU + GPU"
      ],
      "metadata": {
        "id": "AJEgsMbPCxHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hybrid.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <chrono>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// CUDA-ядро для суммирования части массива\n",
        "__global__ void sumKernel(int* d_a, int* d_sum, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n) {\n",
        "        atomicAdd(d_sum, d_a[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Размер массива\n",
        "    const int N = 1'000'000;\n",
        "\n",
        "    // Половина массива для гибридной обработки\n",
        "    const int half = N / 2;\n",
        "\n",
        "    // Инициализация массива на host\n",
        "    std::vector<int> a(N, 1);\n",
        "\n",
        "    // Фиксация времени начала гибридных вычислений\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // --- CPU: обработка первой половины массива ---\n",
        "    long long cpu_part = 0;\n",
        "    for (int i = 0; i < half; i++)\n",
        "        cpu_part += a[i];\n",
        "\n",
        "    // --- GPU: обработка второй половины массива ---\n",
        "    int size = half * sizeof(int);\n",
        "    int *d_a, *d_sum;\n",
        "\n",
        "    // Выделение памяти на GPU\n",
        "    cudaMalloc(&d_a, size);\n",
        "    cudaMalloc(&d_sum, sizeof(int));\n",
        "\n",
        "    // Копирование второй половины массива на device\n",
        "    cudaMemcpy(d_a, a.data() + half, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Инициализация суммы нулём\n",
        "    cudaMemset(d_sum, 0, sizeof(int));\n",
        "\n",
        "    // Параметры запуска CUDA-ядра\n",
        "    int threads = 256;\n",
        "    int blocks = (half + threads - 1) / threads;\n",
        "\n",
        "    // Запуск CUDA-ядра для второй половины массива\n",
        "    sumKernel<<<blocks, threads>>>(d_a, d_sum, half);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Копирование результата GPU-части на host\n",
        "    int gpu_part;\n",
        "    cudaMemcpy(&gpu_part, d_sum, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Фиксация времени окончания вычислений\n",
        "    auto end = std::chrono::high_resolution_clock::now();\n",
        "    std::chrono::duration<double, std::milli> time = end - start;\n",
        "\n",
        "    // Объединение результатов CPU и GPU\n",
        "    long long total = cpu_part + gpu_part;\n",
        "\n",
        "    // Вывод результата и времени\n",
        "    std::cout << \"Hybrid sum: \" << total << std::endl;\n",
        "    std::cout << \"Hybrid time: \" << time.count() << \" ms\" << std::endl;\n",
        "\n",
        "    // Освобождение памяти\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_sum);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfmIytmX902s",
        "outputId": "0746da55-8fcc-41dd-b007-8c6584d93f79"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hybrid.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc hybrid.cu -arch=compute_75 -code=sm_75 -o hybrid\n",
        "!./hybrid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDYBrEVw-g6w",
        "outputId": "6a2a4f48-2927-434d-ad53-db83245e40a0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid sum: 1000000\n",
            "Hybrid time: 206.595 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Все три реализации корректно вычисляют сумму элементов массива, что подтверждается совпадением результатов. Последовательная реализация на CPU показала время выполнения 17.23 мс, тогда как реализация на GPU выполнилась значительно быстрее — за 0.16 мс благодаря массовому параллелизму.\n",
        "\n",
        "Гибридная реализация оказалась существенно медленнее и выполнилась за 206.6 мс. Это объясняется большими накладными расходами на передачу данных между CPU и GPU, а также последовательным характером выполнения частей алгоритма. В данной задаче вычислительная нагрузка слишком мала, чтобы компенсировать стоимость копирования данных и синхронизации.\n",
        "\n",
        "Полученные результаты показывают, что гибридный подход не всегда приводит к ускорению и эффективен только при достаточно сложных вычислениях или при перекрытии вычислений и обмена данными."
      ],
      "metadata": {
        "id": "XMlAEEZKCcT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 4"
      ],
      "metadata": {
        "id": "nH-ydvMT_RfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mpi_sum.cpp\n",
        "\n",
        "#include <mpi.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    // Инициализация MPI-среды\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank, size;\n",
        "    // Получение номера текущего процесса\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    // Получение общего числа процессов\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    // Размер обрабатываемого массива\n",
        "    const int N = 1'000'000;\n",
        "\n",
        "    // Размер части массива для каждого процесса\n",
        "    int chunk_size = N / size;\n",
        "\n",
        "    // Основной массив (используется только процессом с rank 0)\n",
        "    std::vector<int> data;\n",
        "\n",
        "    // Локальная часть массива для каждого процесса\n",
        "    std::vector<int> local_data(chunk_size);\n",
        "\n",
        "    // Инициализация массива только на главном процессе\n",
        "    if (rank == 0) {\n",
        "        data.resize(N, 1);\n",
        "    }\n",
        "\n",
        "    // Фиксация времени начала выполнения\n",
        "    double start_time = MPI_Wtime();\n",
        "\n",
        "    // Распределение массива между процессами\n",
        "    // Каждый процесс получает chunk_size элементов\n",
        "    MPI_Scatter(\n",
        "        data.data(),            // исходный массив (только у rank 0)\n",
        "        chunk_size, MPI_INT,    // количество и тип отправляемых данных\n",
        "        local_data.data(),      // локальный массив\n",
        "        chunk_size, MPI_INT,    // количество и тип принимаемых данных\n",
        "        0,                      // root-процесс\n",
        "        MPI_COMM_WORLD\n",
        "    );\n",
        "\n",
        "    // Локальное вычисление суммы элементов\n",
        "    long long local_sum = 0;\n",
        "    for (int value : local_data) {\n",
        "        local_sum += value;\n",
        "    }\n",
        "\n",
        "    // Переменная для хранения итоговой суммы\n",
        "    long long global_sum = 0;\n",
        "\n",
        "    // Сбор локальных сумм со всех процессов\n",
        "    MPI_Reduce(\n",
        "        &local_sum,             // локальное значение\n",
        "        &global_sum,            // итоговое значение (у rank 0)\n",
        "        1,\n",
        "        MPI_LONG_LONG,\n",
        "        MPI_SUM,\n",
        "        0,\n",
        "        MPI_COMM_WORLD\n",
        "    );\n",
        "\n",
        "    // Фиксация времени окончания выполнения\n",
        "    double end_time = MPI_Wtime();\n",
        "\n",
        "    // Вывод результатов только главным процессом\n",
        "    if (rank == 0) {\n",
        "        std::cout << \"MPI processes: \" << size << std::endl;\n",
        "        std::cout << \"Global sum: \" << global_sum << std::endl;\n",
        "        std::cout << \"Execution time: \"\n",
        "                  << (end_time - start_time) * 1000\n",
        "                  << \" ms\" << std::endl;\n",
        "    }\n",
        "\n",
        "    // Завершение MPI-среды\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Us_CTUeD_WjU",
        "outputId": "9af1060c-6e0a-4d57-cf37-a25ab1930d97"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mpi_sum.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpic++ mpi_sum.cpp -o mpi_sum"
      ],
      "metadata": {
        "id": "0bJwSRip_efH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./mpi_sum\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 4 ./mpi_sum\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 8 ./mpi_sum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv--ZzOl_0h0",
        "outputId": "0a8b1b57-c082-4efe-f936-e733048b857c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MPI processes: 2\n",
            "Global sum: 1000000\n",
            "Execution time: 11.1409 ms\n",
            "MPI processes: 4\n",
            "Global sum: 1000000\n",
            "Execution time: 19.5684 ms\n",
            "MPI processes: 8\n",
            "Global sum: 1000000\n",
            "Execution time: 12.963 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Распределённая MPI-программа корректно вычисляет сумму элементов массива, что подтверждается совпадением результата при использовании 2, 4 и 8 процессов. При этом время выполнения не уменьшается с ростом числа процессов: при 2 процессах время составило 11.14 мс, при 4 — увеличилось до 19.57 мс, а при 8 — составило 12.96 мс.\n",
        "\n",
        "Такое поведение объясняется тем, что вычисления выполнялись на одном узле с ограниченным числом CPU-ядер, а запуск большего числа MPI-процессов происходил в режиме oversubscribe. В результате накладные расходы на создание процессов, обмен данными и синхронизацию превысили выигрыш от параллелизма.\n",
        "\n",
        "Полученные результаты показывают, что эффективность MPI-параллелизма напрямую зависит от аппаратных ресурсов и что распределённые вычисления оправданы в первую очередь для крупных задач и многопроцессорных систем, а не для небольших массивов на одной машине."
      ],
      "metadata": {
        "id": "MC_cEb9rAVvl"
      }
    }
  ]
}