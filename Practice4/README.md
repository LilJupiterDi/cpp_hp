# Practice 4


## 1. Чем отличаются типы памяти в CUDA и в каких случаях их использовать?

В CUDA используются несколько типов памяти, отличающихся по скорости доступа и области видимости.
Глобальная память имеет большой объём и доступна всем потокам, но отличается высокой задержкой доступа, поэтому применяется для хранения входных и выходных данных.
Разделяемая память является общей для потоков одного блока и имеет значительно меньшую задержку, что делает её эффективной для хранения промежуточных результатов и обмена данными между потоками.
Локальная память доступна только одному потоку и обычно размещается в регистрах, используется для временных переменных и вычислений внутри потока.

---

## 2. Как использование разделяемой памяти влияет на производительность?

Использование разделяемой памяти позволяет значительно повысить производительность за счёт уменьшения количества обращений к глобальной памяти. Поскольку разделяемая память имеет низкую задержку доступа, операции чтения и записи выполняются быстрее. Это особенно эффективно в алгоритмах, где данные многократно используются несколькими потоками внутри одного блока.

---

## 3. Что такое согласованный доступ к памяти и как его обеспечить?

Согласованный (coalesced) доступ к памяти означает, что потоки одного варпа обращаются к последовательным адресам глобальной памяти. Это позволяет GPU объединять такие обращения в меньшее количество транзакций, повышая пропускную способность. Обеспечить согласованный доступ можно за счёт правильной организации данных в памяти и корректного сопоставления индексов потоков с элементами массива.

---

## 4. Какие сложности возникают при работе с большим объёмом данных на GPU?

Основные сложности связаны с ограниченным объёмом и пропускной способностью памяти GPU, высокой задержкой доступа к глобальной памяти и необходимостью эффективной синхронизации потоков. Также возникают дополнительные накладные расходы при передаче данных между CPU и GPU, что может снижать общую производительность при неэффективной организации вычислений.

---

## 5. Почему важно минимизировать доступ к глобальной памяти?

Глобальная память является самой медленной памятью в иерархии CUDA. Частые обращения к ней значительно увеличивают время выполнения программы. Минимизация доступа к глобальной памяти и перенос вычислений в разделяемую или локальную память позволяет снизить задержки и повысить эффективность параллельных вычислений.

---

## 6. Как использовать профилирование для анализа производительности CUDA-программ?

Для анализа производительности CUDA-программ используются инструменты профилирования, такие как **NVIDIA Nsight Systems** и **Nsight Compute**. Они позволяют измерять время выполнения kernel’ов, выявлять узкие места, анализировать использование памяти и загрузку вычислительных блоков. На основе результатов профилирования можно оптимизировать распределение потоков, использование памяти и структуру алгоритма.
