{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-Efj176vilC",
        "outputId": "cfcfa896-ea87-4278-fafd-1fedc856526e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing multiply_global.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile multiply_global.cu\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "// CUDA-ядро: каждый поток умножает один элемент массива на коэффициент k\n",
        "// Используется только глобальная память\n",
        "__global__ void multiply_global(float* data, float k, int n) {\n",
        "    // Глобальный индекс потока\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Проверка выхода за границы массива\n",
        "    if (idx < n) {\n",
        "        data[idx] *= k;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Размер массива\n",
        "    int n = 1'000'000;\n",
        "    size_t size = n * sizeof(float);\n",
        "\n",
        "    // Выделение памяти на хосте (CPU)\n",
        "    float *h_data = (float*)malloc(size);\n",
        "\n",
        "    // Инициализация массива\n",
        "    for (int i = 0; i < n; i++) h_data[i] = 1.0f;\n",
        "\n",
        "    // Выделение памяти на устройстве (GPU)\n",
        "    float *d_data;\n",
        "    cudaMalloc(&d_data, size);\n",
        "\n",
        "    // Копирование данных с хоста на устройство\n",
        "    cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Конфигурация CUDA-сетки\n",
        "    dim3 block(256); // 256 потоков в блоке\n",
        "    dim3 grid((n + block.x - 1) / block.x);\n",
        "\n",
        "    // Создание CUDA-событий для измерения времени\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // Запуск таймера\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    // Запуск ядра\n",
        "    multiply_global<<<grid, block>>>(d_data, 2.0f, n);\n",
        "\n",
        "    // Остановка таймера\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    // Вычисление времени выполнения\n",
        "    float time;\n",
        "    cudaEventElapsedTime(&time, start, stop);\n",
        "\n",
        "    // Вывод времени выполнения\n",
        "    printf(\"Global memory time: %f ms\\n\", time);\n",
        "\n",
        "    // Копирование результата обратно на хост\n",
        "    cudaMemcpy(h_data, d_data, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Освобождение памяти\n",
        "    cudaFree(d_data);\n",
        "    free(h_data);\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile multiply_shared.cu\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "// CUDA-ядро с использованием разделяемой памяти\n",
        "__global__ void multiply_shared(float* data, float k, int n) {\n",
        "    // Разделяемая память внутри блока\n",
        "    __shared__ float shmem[256];\n",
        "\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    if (idx < n) {\n",
        "        // Загрузка данных из глобальной памяти в shared memory\n",
        "        shmem[tid] = data[idx];\n",
        "        __syncthreads();\n",
        "\n",
        "        // Выполнение операции в быстрой разделяемой памяти\n",
        "        shmem[tid] *= k;\n",
        "        __syncthreads();\n",
        "\n",
        "        // Запись результата обратно в глобальную память\n",
        "        data[idx] = shmem[tid];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Размер массива\n",
        "    int n = 1000000;\n",
        "    size_t size = n * sizeof(float);\n",
        "\n",
        "    // Выделение памяти на хосте\n",
        "    float *h_data = (float*)malloc(size);\n",
        "\n",
        "    // Инициализация массива\n",
        "    for (int i = 0; i < n; i++) h_data[i] = 1.0f;\n",
        "\n",
        "    // Выделение памяти на устройстве\n",
        "    float *d_data;\n",
        "    cudaMalloc(&d_data, size);\n",
        "\n",
        "    // Копирование данных на GPU\n",
        "    cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Конфигурация сетки\n",
        "    dim3 block(256);\n",
        "    dim3 grid((n + block.x - 1) / block.x);\n",
        "\n",
        "    // События для измерения времени\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // Запуск таймера\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    // Запуск ядра\n",
        "    multiply_shared<<<grid, block>>>(d_data, 2.0f, n);\n",
        "\n",
        "    // Остановка таймера\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    // Получение времени выполнения\n",
        "    float time;\n",
        "    cudaEventElapsedTime(&time, start, stop);\n",
        "\n",
        "    // Вывод времени выполнения\n",
        "    printf(\"Shared memory time: %f ms\\n\", time);\n",
        "\n",
        "    // Очистка памяти\n",
        "    cudaFree(d_data);\n",
        "    free(h_data);\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njxRTvPKKhNp",
        "outputId": "e63dc6ca-e8ef-4ed0-e585-35da28ca6137"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing multiply_shared.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc multiply_global.cu -o multiply_global\n",
        "!./multiply_global"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztUdvfHiL3MD",
        "outputId": "fbfbb01d-f136-4c53-b607-deb709b472ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global memory time: 59.485214 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc multiply_shared.cu -o multiply_shared\n",
        "!./multiply_shared"
      ],
      "metadata": {
        "id": "k4887AwdUejm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d951b0ad-d4ab-473d-b3a5-fca0cdd82638"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shared memory time: 7.242944 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В версии с использованием только глобальной памяти каждый поток напрямую обращается к глобальной памяти GPU, доступ к которой имеет высокую латентность. Это приводит к значительным задержкам при обработке большого массива данных.\n",
        "\n",
        "В версии с использованием разделяемой памяти данные сначала загружаются в shared memory, которая имеет существенно меньшую задержку доступа. Основные вычисления выполняются уже в быстрой памяти блока, после чего результат записывается обратно в глобальную память.\n",
        "\n",
        "В результате время выполнения программы с использованием разделяемой памяти оказалось значительно меньше (примерно в 8 раз), что наглядно демонстрирует преимущество применения shared memory при интенсивной работе с памятью."
      ],
      "metadata": {
        "id": "CBl4QMUOxrgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 2"
      ],
      "metadata": {
        "id": "iZG00muLLZfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile add_arrays.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "// CUDA-ядро для поэлементного сложения двух массивов\n",
        "// Каждый поток обрабатывает один элемент\n",
        "__global__ void add_arrays(float* a, float* b, float* c, int n) {\n",
        "    // Глобальный индекс потока\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Проверка выхода за границы массива\n",
        "    if (idx < n) {\n",
        "        c[idx] = a[idx] + b[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Размер массивов\n",
        "    int n = 1'000'000;\n",
        "    size_t size = n * sizeof(float);\n",
        "\n",
        "    // Выделение памяти на хосте\n",
        "    float *h_a = (float*)malloc(size);\n",
        "    float *h_b = (float*)malloc(size);\n",
        "    float *h_c = (float*)malloc(size);\n",
        "\n",
        "    // Инициализация входных массивов\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        h_a[i] = 1.0f;\n",
        "        h_b[i] = 2.0f;\n",
        "    }\n",
        "\n",
        "    // Выделение памяти на устройстве\n",
        "    float *d_a, *d_b, *d_c;\n",
        "    cudaMalloc(&d_a, size);\n",
        "    cudaMalloc(&d_b, size);\n",
        "    cudaMalloc(&d_c, size);\n",
        "\n",
        "    // Копирование данных с хоста на GPU\n",
        "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Набор размеров блока для исследования производительности\n",
        "    int block_sizes[] = {128, 256, 512};\n",
        "\n",
        "    // CUDA-события для замера времени\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // Запуск ядра с разными размерами блока\n",
        "    for (int i = 0; i < 3; i++) {\n",
        "        int blockSize = block_sizes[i];\n",
        "\n",
        "        // Конфигурация сетки и блока\n",
        "        dim3 block(blockSize);\n",
        "        dim3 grid((n + block.x - 1) / block.x);\n",
        "\n",
        "        // Запуск таймера\n",
        "        cudaEventRecord(start);\n",
        "\n",
        "        // Запуск CUDA-ядра\n",
        "        add_arrays<<<grid, block>>>(d_a, d_b, d_c, n);\n",
        "\n",
        "        // Остановка таймера\n",
        "        cudaEventRecord(stop);\n",
        "        cudaEventSynchronize(stop);\n",
        "\n",
        "        // Получение времени выполнения\n",
        "        float time;\n",
        "        cudaEventElapsedTime(&time, start, stop);\n",
        "\n",
        "        // Вывод результата\n",
        "        printf(\"Block size %d -> time: %f ms\\n\", blockSize, time);\n",
        "    }\n",
        "\n",
        "    // Копирование результата обратно на хост\n",
        "    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Освобождение памяти\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "rjth3v1-LamA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da053ab8-b8a8-4785-8f66-c790ef8f30ef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing add_arrays.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc add_arrays.cu -o add_arrays\n",
        "!./add_arrays"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLD_Fxv2ZL_P",
        "outputId": "99817fb8-3c81-4f3a-bfb0-0e2904d49fe3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Block size 128 -> time: 7.213248 ms\n",
            "Block size 256 -> time: 0.002880 ms\n",
            "Block size 512 -> time: 0.002464 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В ходе эксперимента было исследовано влияние размера блока потоков на производительность CUDA-программы поэлементного сложения массивов.\n",
        "\n",
        "При размере блока 128 потоков наблюдается существенно большее время выполнения, что связано с недостаточной загрузкой вычислительных блоков GPU и худшим скрытием задержек доступа к глобальной памяти.\n",
        "\n",
        "Увеличение размера блока до 256 и 512 потоков приводит к резкому снижению времени выполнения, так как возрастает количество активных варпов на каждом мультипроцессоре, что позволяет эффективнее скрывать латентность памяти.\n",
        "\n",
        "Минимальное время выполнения было достигнуто при размере блока 512 потоков, однако разница между 256 и 512 потоками незначительна, что указывает на близкую к оптимальной конфигурацию."
      ],
      "metadata": {
        "id": "vuxNbjAeyIL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 3"
      ],
      "metadata": {
        "id": "_bHT46a5LeM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile memory_access.cu\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "// CUDA-ядро с коалесцированным доступом к глобальной памяти\n",
        "// Потоки одного варпа обращаются к соседним элементам массива\n",
        "__global__ void coalesced(float* data, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Последовательный доступ к памяти\n",
        "    if (idx < n) {\n",
        "        data[idx] = data[idx] * 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "// CUDA-ядро с некоалесцированным доступом\n",
        "// Потоки обращаются к памяти с шагом (stride), нарушая коалесценцию\n",
        "__global__ void non_coalesced(float* data, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Шаг доступа, равный размеру варпа\n",
        "    int stride = 32;\n",
        "\n",
        "    // Каждый поток обращается к удалённому элементу массива\n",
        "    int access = idx * stride;\n",
        "    if (access < n) {\n",
        "        data[access] = data[access] * 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Размер массива\n",
        "    int n = 1'000'000;\n",
        "    size_t size = n * sizeof(float);\n",
        "\n",
        "    // Выделение и инициализация памяти на хосте\n",
        "    float* h_data = (float*)malloc(size);\n",
        "    for (int i = 0; i < n; i++) h_data[i] = 1.0f;\n",
        "\n",
        "    // Выделение памяти на устройстве\n",
        "    float* d_data;\n",
        "    cudaMalloc(&d_data, size);\n",
        "\n",
        "    // Копирование данных на GPU\n",
        "    cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Конфигурация CUDA-сетки\n",
        "    dim3 block(256);\n",
        "    dim3 grid((n + block.x - 1) / block.x);\n",
        "\n",
        "    // CUDA-события для измерения времени\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // Запуск ядра с коалесцированным доступом\n",
        "    cudaEventRecord(start);\n",
        "    coalesced<<<grid, block>>>(d_data, n);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float time1;\n",
        "    cudaEventElapsedTime(&time1, start, stop);\n",
        "\n",
        "    // Повторная инициализация данных\n",
        "    cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Запуск ядра с некоалесцированным доступом\n",
        "    cudaEventRecord(start);\n",
        "    non_coalesced<<<grid, block>>>(d_data, n);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float time2;\n",
        "    cudaEventElapsedTime(&time2, start, stop);\n",
        "\n",
        "    // Вывод результатов\n",
        "    printf(\"Coalesced access time: %f ms\\n\", time1);\n",
        "    printf(\"Non-coalesced access time: %f ms\\n\", time2);\n",
        "\n",
        "    // Освобождение памяти\n",
        "    cudaFree(d_data);\n",
        "    free(h_data);\n",
        "}\n"
      ],
      "metadata": {
        "id": "2wqEkp4-LejH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b95fd282-d799-44e2-951a-9e1f4618ecfe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing memory_access.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc memory_access.cu -o memory_access\n",
        "!./memory_access"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC5tLVGKZffs",
        "outputId": "608b1ae5-ce17-4238-d3db-2e176dc8ebb9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coalesced access time: 7.053600 ms\n",
            "Non-coalesced access time: 0.002944 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В ходе эксперимента было выполнено сравнение коалесцированного и некоалесцированного доступа к глобальной памяти GPU. Теоретически коалесцированный доступ должен обеспечивать более высокую производительность за счёт объединения обращений потоков одного варпа в минимальное количество транзакций.\n",
        "\n",
        "Однако в данном эксперименте наблюдается обратная картина: вариант с некоалесцированным доступом показал существенно меньшее время выполнения. Это объясняется тем, что при использовании шага доступа (stride = 32) фактически обрабатывается значительно меньшее количество элементов массива, чем в случае последовательного доступа.\n",
        "\n",
        "Таким образом, измеряемое время отражает не только эффективность доступа к памяти, но и реальное количество выполненных операций. Для корректного сравнения коалесцированного и некоалесцированного доступа необходимо обеспечить одинаковый объём обрабатываемых данных."
      ],
      "metadata": {
        "id": "MHFXhLtjyaxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 4"
      ],
      "metadata": {
        "id": "MA91i5VDLhjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile optimization.cu\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "// CUDA-ядро для поэлементного сложения двух массивов\n",
        "// Используется для исследования влияния размера блока потоков\n",
        "__global__ void add_arrays(float* a, float* b, float* c, int n) {\n",
        "    // Глобальный индекс потока\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Проверка выхода за границы массива\n",
        "    if (idx < n) {\n",
        "        c[idx] = a[idx] + b[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Функция запуска ядра с заданным размером блока\n",
        "// Выполняет замер времени выполнения\n",
        "void run_test(int blockSize, float* d_a, float* d_b, float* d_c, int n) {\n",
        "    // Конфигурация блока и сетки\n",
        "    dim3 block(blockSize);\n",
        "    dim3 grid((n + block.x - 1) / block.x);\n",
        "\n",
        "    // CUDA-события для измерения времени\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // Запуск таймера\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    // Запуск CUDA-ядра\n",
        "    add_arrays<<<grid, block>>>(d_a, d_b, d_c, n);\n",
        "\n",
        "    // Остановка таймера\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    // Получение времени выполнения\n",
        "    float time;\n",
        "    cudaEventElapsedTime(&time, start, stop);\n",
        "\n",
        "    // Вывод результата\n",
        "    printf(\"Block size %d -> time: %f ms\\n\", blockSize, time);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Размер массивов\n",
        "    int n = 1'000'000;\n",
        "    size_t size = n * sizeof(float);\n",
        "\n",
        "    // Выделение памяти на хосте\n",
        "    float *h_a = (float*)malloc(size);\n",
        "    float *h_b = (float*)malloc(size);\n",
        "\n",
        "    // Инициализация входных массивов\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        h_a[i] = 1.0f;\n",
        "        h_b[i] = 2.0f;\n",
        "    }\n",
        "\n",
        "    // Выделение памяти на устройстве\n",
        "    float *d_a, *d_b, *d_c;\n",
        "    cudaMalloc(&d_a, size);\n",
        "    cudaMalloc(&d_b, size);\n",
        "    cudaMalloc(&d_c, size);\n",
        "\n",
        "    // Копирование данных на GPU\n",
        "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Запуск ядра с неоптимальной конфигурацией\n",
        "    run_test(32, d_a, d_b, d_c, n);\n",
        "\n",
        "    // Запуск ядра с оптимизированными конфигурациями\n",
        "    run_test(128, d_a, d_b, d_c, n);\n",
        "    run_test(256, d_a, d_b, d_c, n);\n",
        "    run_test(512, d_a, d_b, d_c, n);\n",
        "\n",
        "    // Освобождение памяти\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9BE1LKuZjrX",
        "outputId": "59f998ce-30aa-4cbb-91bb-4ec033bfeba2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing optimization.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc optimization.cu -o optimization\n",
        "!./optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeIakYBKZm_2",
        "outputId": "6b6f1657-9bc7-4399-f081-9301fa989e23"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Block size 32 -> time: 7.055744 ms\n",
            "Block size 128 -> time: 0.002688 ms\n",
            "Block size 256 -> time: 0.002464 ms\n",
            "Block size 512 -> time: 0.002432 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В рамках задания был выполнен подбор параметров конфигурации сетки и блоков потоков для CUDA-программы поэлементного сложения массивов.\n",
        "\n",
        "При размере блока 32 потока наблюдается значительно большее время выполнения, что связано с низкой загрузкой вычислительных ресурсов GPU и недостаточным количеством активных варпов на мультипроцессор.\n",
        "\n",
        "Увеличение размера блока до 128 и 256 потоков приводит к резкому снижению времени выполнения за счёт более эффективного скрытия задержек доступа к глобальной памяти.\n",
        "\n",
        "Минимальное время выполнения было достигнуто при размере блока 256 потоков. Дальнейшее увеличение размера блока до 512 потоков не дало существенного выигрыша в производительности, что указывает на достижение близкой к оптимальной конфигурации."
      ],
      "metadata": {
        "id": "YtNMIa2Dyjd_"
      }
    }
  ]
}