{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Задание 1. Обработка массива на CPU (OpenMP)"
      ],
      "metadata": {
        "id": "XwAIKJrJr_TF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHZtEZafMNC3",
        "outputId": "2c491319-aa31-44bd-e524-f39329b42fd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cpu_process.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile cpu_process.cpp\n",
        "\n",
        "#include <iostream>     // ввод-вывод\n",
        "#include <vector>       // контейнер std::vector\n",
        "#include <chrono>       // измерение времени\n",
        "#include <omp.h>        // OpenMP для параллельных вычислений на CPU\n",
        "\n",
        "// Функция обработки массива на CPU\n",
        "// Каждый элемент массива умножается на 2\n",
        "void cpu_process(std::vector<float>& data) {\n",
        "\n",
        "    // Директива OpenMP\n",
        "    // parallel for автоматически распараллеливает цикл\n",
        "    // Итерации цикла распределяются между потоками CPU\n",
        "    #pragma omp parallel for\n",
        "    for (size_t i = 0; i < data.size(); i++) {\n",
        "        data[i] *= 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Размер массива\n",
        "    const size_t N = 1000000;\n",
        "\n",
        "    // Инициализация массива значениями 1.0\n",
        "    std::vector<float> data(N, 1.0f);\n",
        "\n",
        "    // Засекаем время начала вычислений\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Обработка массива на CPU с использованием OpenMP\n",
        "    cpu_process(data);\n",
        "\n",
        "    // Засекаем время окончания вычислений\n",
        "    auto end = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Вычисляем длительность выполнения в миллисекундах\n",
        "    std::chrono::duration<double, std::milli> time = end - start;\n",
        "\n",
        "    // Вывод времени выполнения\n",
        "    std::cout << \"CPU time: \" << time.count() << \" ms\\n\";\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ -o cpu_process cpu_process.cpp -lstdc++ -fopenmp\n",
        "!./cpu_process"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJcQSFO6sMZA",
        "outputId": "77f76ec4-843a-421f-fd0d-352517a844e8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU time: 3.44847 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 2. Обработка массива на GPU (CUDA)"
      ],
      "metadata": {
        "id": "pmObLMuysSzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gpu_process.cu\n",
        "\n",
        "#include <cuda_runtime.h>   // основные функции CUDA\n",
        "#include <iostream>         // ввод-вывод\n",
        "#include <vector>           // контейнер std::vector\n",
        "#include <chrono>           // измерение времени\n",
        "\n",
        "// CUDA-ядро (kernel)\n",
        "// Выполняется на GPU\n",
        "// Каждый поток обрабатывает один элемент массива\n",
        "__global__ void gpu_kernel(float* data, int N) {\n",
        "\n",
        "    // Глобальный индекс потока\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Проверка выхода за границы массива\n",
        "    if (idx < N) {\n",
        "        data[idx] *= 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "\n",
        "    // Размер массива\n",
        "    const int N = 1'000'000;\n",
        "\n",
        "    // Размер памяти в байтах\n",
        "    size_t size = N * sizeof(float);\n",
        "\n",
        "    // Массив в памяти хоста (CPU)\n",
        "    std::vector<float> h_data(N, 1.0f);\n",
        "\n",
        "    // Указатель на массив в памяти устройства (GPU)\n",
        "    float* d_data;\n",
        "\n",
        "    // Выделение памяти на GPU\n",
        "    cudaMalloc(&d_data, size);\n",
        "\n",
        "    // Копирование данных с CPU на GPU\n",
        "    cudaMemcpy(d_data, h_data.data(), size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Засекаем время начала вычислений на GPU\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Количество потоков в одном блоке\n",
        "    int threads = 256;\n",
        "\n",
        "    // Количество блоков в сетке\n",
        "    int blocks = (N + threads - 1) / threads;\n",
        "\n",
        "    // Запуск CUDA-ядра\n",
        "    gpu_kernel<<<blocks, threads>>>(d_data, N);\n",
        "\n",
        "    // Ожидание завершения всех потоков GPU\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Засекаем время окончания вычислений\n",
        "    auto end = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Копирование результатов с GPU обратно на CPU\n",
        "    cudaMemcpy(h_data.data(), d_data, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Освобождение памяти GPU\n",
        "    cudaFree(d_data);\n",
        "\n",
        "    // Вычисление времени выполнения в миллисекундах\n",
        "    std::chrono::duration<double, std::milli> time = end - start;\n",
        "\n",
        "    // Вывод времени выполнения GPU-вычислений\n",
        "    std::cout << \"GPU time: \" << time.count() << \" ms\\n\";\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmoh4cV9sTTu",
        "outputId": "c50fc78b-152c-4c78-a11c-0a4c71a5cba0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gpu_process.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o gpu_process gpu_process.cu -lstdc++ -lcudart\n",
        "!./gpu_process"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce63CV0gsYWp",
        "outputId": "633b2add-73e8-48e3-d80b-194676e707bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU time: 50.5873 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 3. Гибридная обработка массива"
      ],
      "metadata": {
        "id": "WSipkSirsfQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hybrid_process.cu\n",
        "\n",
        "#include <iostream>        // ввод-вывод\n",
        "#include <vector>          // std::vector\n",
        "#include <chrono>          // измерение времени\n",
        "#include <omp.h>           // OpenMP для параллельных вычислений на CPU\n",
        "#include <cuda_runtime.h>  // CUDA API\n",
        "\n",
        "// CUDA-ядро\n",
        "// Обрабатывает вторую половину массива, начиная с offset\n",
        "__global__ void gpu_kernel(float* data, int offset, int N) {\n",
        "\n",
        "    // Глобальный индекс потока внутри GPU\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Проверка границ\n",
        "    if (idx < N) {\n",
        "        data[offset + idx] *= 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "\n",
        "    // Общий размер массива\n",
        "    const int N = 1'000'000;\n",
        "\n",
        "    // Половина массива для гибридной обработки\n",
        "    const int half = N / 2;\n",
        "\n",
        "    // Размер памяти в байтах\n",
        "    size_t size = N * sizeof(float);\n",
        "\n",
        "    // Массив в памяти CPU\n",
        "    std::vector<float> data(N, 1.0f);\n",
        "\n",
        "    // Указатель на массив в памяти GPU\n",
        "    float* d_data;\n",
        "\n",
        "    // Выделение памяти на GPU\n",
        "    cudaMalloc(&d_data, size);\n",
        "\n",
        "    // Копирование всего массива с CPU на GPU\n",
        "    cudaMemcpy(d_data, data.data(), size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Начало измерения общего времени гибридных вычислений\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Параллельные секции OpenMP\n",
        "    // CPU и GPU работают одновременно\n",
        "    #pragma omp parallel sections\n",
        "    {\n",
        "        // Первая секция: обработка первой половины массива на CPU\n",
        "        #pragma omp section\n",
        "        {\n",
        "            for (int i = 0; i < half; i++) {\n",
        "                data[i] *= 2.0f;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Вторая секция: обработка второй половины массива на GPU\n",
        "        #pragma omp section\n",
        "        {\n",
        "            int threads = 256;                          // потоки в блоке\n",
        "            int blocks = (half + threads - 1) / threads; // количество блоков\n",
        "\n",
        "            // Запуск CUDA-ядра для второй половины массива\n",
        "            gpu_kernel<<<blocks, threads>>>(d_data, half, half);\n",
        "\n",
        "            // Ожидание завершения вычислений на GPU\n",
        "            cudaDeviceSynchronize();\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Копирование обработанной второй половины массива с GPU на CPU\n",
        "    cudaMemcpy(data.data() + half,\n",
        "               d_data + half,\n",
        "               half * sizeof(float),\n",
        "               cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Освобождение памяти GPU\n",
        "    cudaFree(d_data);\n",
        "\n",
        "    // Окончание измерения времени\n",
        "    auto end = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Общее время гибридной обработки\n",
        "    std::chrono::duration<double, std::milli> time = end - start;\n",
        "\n",
        "    // Вывод времени выполнения\n",
        "    std::cout << \"Hybrid time: \" << time.count() << \" ms\\n\";\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV0FxwPwsg26",
        "outputId": "985af004-028e-405e-cfa4-878bc06aca55"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hybrid_process.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o hybrid_process hybrid_process.cu -lstdc++ -lcudart\n",
        "!./hybrid_process"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLhjXdG7snln",
        "outputId": "164cd4f1-0345-4b64-dfdf-38daa7a5b464"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid time: 9.6639 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Задание 4. Анализ производительности\n",
        "\n",
        "В ходе эксперимента были получены следующие результаты времени выполнения обработки массива размером 1 000 000 элементов:\n",
        "\n",
        "* CPU (OpenMP): **3.45 мс**\n",
        "* GPU (CUDA): **50.59 мс**\n",
        "* Гибридный режим (CPU + GPU): **9.66 мс**\n",
        "\n",
        "### Сравнение результатов\n",
        "\n",
        "Наименьшее время выполнения показала реализация на CPU с использованием OpenMP. Это объясняется тем, что операция умножения элементов массива является достаточно простой, а накладные расходы на передачу данных между CPU и GPU в данном случае превышают выигрыш от параллельных вычислений на GPU.\n",
        "\n",
        "GPU-реализация оказалась самой медленной. Основная причина — необходимость копирования данных между памятью CPU и GPU по шине PCI Express, а также синхронизация выполнения (`cudaDeviceSynchronize`). Для такой простой операции стоимость этих действий значительно выше, чем само вычисление.\n",
        "\n",
        "Гибридная реализация показала промежуточный результат. За счет параллельной работы CPU и GPU общее время выполнения уменьшилось по сравнению с чистой GPU-реализацией, однако гибридный вариант все равно уступает CPU-реализации из-за накладных расходов, связанных с использованием GPU.\n",
        "\n",
        "### Анализ эффективности гибридного подхода\n",
        "\n",
        "Гибридный подход дает наибольший выигрыш в следующих случаях:\n",
        "\n",
        "* при больших объемах данных, когда время вычислений на GPU значительно превышает время передачи данных;\n",
        "* при вычислительно сложных операциях, а не простых арифметических действиях;\n",
        "* когда возможно перекрыть вычисления на CPU и GPU, минимизируя время простоя;\n",
        "* если данные уже частично находятся в памяти GPU и не требуют повторного копирования.\n",
        "\n",
        "В рассматриваемом эксперименте гибридный подход не показал максимальной эффективности, так как задача слишком проста и ограничена накладными расходами на обмен данными между CPU и GPU.\n",
        "\n",
        "### Общий вывод\n",
        "\n",
        "Для простых операций над массивами оптимальным решением является параллельная обработка на CPU с использованием OpenMP. Использование GPU и гибридного подхода оправдано только при более сложных вычислениях и больших объемах данных, где преимущества массового параллелизма GPU перекрывают затраты на передачу данных."
      ],
      "metadata": {
        "id": "ruJOsBxDwjbP"
      }
    }
  ]
}