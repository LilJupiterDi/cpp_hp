# Practice 5

1. Отличие стека и очереди
   Стек работает по принципу последний пришёл – первый вышел. Классика: вызовы функций, откат действий, undo. Ты всегда берёшь элемент с вершины.
   Очередь наоборот: первый пришёл – первый вышел. Типичный пример — очередь задач или пакетов в сети. Важно не только что лежит, но и порядок поступления.

2. Проблемы при параллельном доступе к данным
   Главная беда — гонки данных. Несколько потоков одновременно читают и пишут в одно и то же место, и итог зависит от того, кто успел раньше. Появляются некорректные значения, потерянные обновления, а иногда и совсем странные баги, которые сложно воспроизвести. Плюс возможны дедлоки, когда все ждут друг друга и работа встаёт.

3. Как атомарные операции помогают избежать конфликтов
   Атомарная операция выполняется как неделимое действие. Никто не может вклиниться посередине. Например, atomicAdd гарантирует, что прибавление произойдёт корректно, даже если десятки потоков делают это одновременно. Это спасает от гонок данных, когда нужно обновлять общий счётчик или индекс, не используя тяжёлые блокировки.

4. Типы памяти CUDA для хранения данных
   В CUDA есть несколько уровней памяти. Глобальная память большая, но медленная, доступна всем потокам. Разделяемая память находится внутри блока, очень быстрая, но маленькая. Есть ещё регистры, они самые быстрые и приватные для потока. Плюс константная и текстурная память, которые оптимизированы под специфические сценарии чтения.

5. Влияние синхронизации потоков на производительность
   Синхронизация — это пауза. Пока один поток не дошёл до нужной точки, остальные ждут. Чем чаще и грубее синхронизация, тем больше простаивания и тем ниже производительность. Она необходима для корректности, но злоупотреблять ей нельзя, иначе GPU превращается в дорогой обогреватель.

6. Почему разделяемая память важна для оптимизации
   Разделяемая память в разы быстрее глобальной. Если потоки блока часто работают с одними и теми же данными, выгоднее загрузить их один раз в shared memory и дальше использовать оттуда. Это снижает количество медленных обращений к глобальной памяти и резко ускоряет работу параллельных структур данных. По сути, это локальный кэш, которым ты управляешь сам.
