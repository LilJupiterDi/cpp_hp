{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DM0p779Si_A1",
    "outputId": "3dd03019-b291-4f49-c4ad-26aeeb2e98b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stack_cuda.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile stack_cuda.cu\n",
    "\n",
    "// Include CUDA runtime API (memory management, kernel launch, atomics)\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Include standard C++ input/output stream\n",
    "#include <iostream>\n",
    "\n",
    "// Maximum number of elements the stack can hold\n",
    "#define STACK_CAPACITY 1024\n",
    "\n",
    "// Number of CUDA threads used in kernels\n",
    "#define THREADS 256\n",
    "\n",
    "// ======================\n",
    "// Stack structure definition\n",
    "// ======================\n",
    "\n",
    "// Structure representing a stack in GPU global memory\n",
    "struct Stack {\n",
    "    int data[STACK_CAPACITY]; // Fixed-size array storing stack elements\n",
    "    int top;                  // Index of the next free position (stack pointer)\n",
    "};\n",
    "\n",
    "// ======================\n",
    "// Device function: push\n",
    "// ======================\n",
    "\n",
    "// Pushes a value onto the stack\n",
    "// Returns true if successful, false if stack is full\n",
    "__device__ bool stack_push(Stack* s, int value) {\n",
    "\n",
    "    // Atomically increment stack pointer and get old value\n",
    "    int idx = atomicAdd(&s->top, 1);\n",
    "\n",
    "    // Check for stack overflow\n",
    "    if (idx >= STACK_CAPACITY) {\n",
    "        // Roll back the increment if overflow occurred\n",
    "        atomicSub(&s->top, 1);\n",
    "        return false;\n",
    "    }\n",
    "\n",
    "    // Store value at the allocated stack position\n",
    "    s->data[idx] = value;\n",
    "\n",
    "    return true;\n",
    "}\n",
    "\n",
    "// ======================\n",
    "// Device function: pop\n",
    "// ======================\n",
    "\n",
    "// Pops a value from the stack\n",
    "// Stores popped value in *value\n",
    "// Returns true if successful, false if stack is empty\n",
    "__device__ bool stack_pop(Stack* s, int* value) {\n",
    "\n",
    "    // Atomically decrement stack pointer and get previous index\n",
    "    int idx = atomicSub(&s->top, 1) - 1;\n",
    "\n",
    "    // Check for stack underflow\n",
    "    if (idx < 0) {\n",
    "        // Roll back decrement if stack was empty\n",
    "        atomicAdd(&s->top, 1);\n",
    "        return false;\n",
    "    }\n",
    "\n",
    "    // Retrieve value from stack\n",
    "    *value = s->data[idx];\n",
    "\n",
    "    return true;\n",
    "}\n",
    "\n",
    "// ======================\n",
    "// Kernel: parallel push\n",
    "// ======================\n",
    "\n",
    "// CUDA kernel where multiple threads push values concurrently\n",
    "__global__ void push_kernel(Stack* s) {\n",
    "\n",
    "    // Compute global thread index\n",
    "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "\n",
    "    // Each thread attempts to push its thread ID\n",
    "    stack_push(s, tid);\n",
    "}\n",
    "\n",
    "// ======================\n",
    "// Kernel: parallel pop\n",
    "// ======================\n",
    "\n",
    "// CUDA kernel where multiple threads pop values concurrently\n",
    "__global__ void pop_kernel(Stack* s, int* output) {\n",
    "\n",
    "    // Compute global thread index\n",
    "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "\n",
    "    int value;\n",
    "\n",
    "    // Attempt to pop from stack\n",
    "    if (stack_pop(s, &value)) {\n",
    "        // Store popped value if successful\n",
    "        output[tid] = value;\n",
    "    } else {\n",
    "        // Mark failed pop (stack empty)\n",
    "        output[tid] = -1;\n",
    "    }\n",
    "}\n",
    "\n",
    "// ======================\n",
    "// Main function (host code)\n",
    "// ======================\n",
    "\n",
    "int main() {\n",
    "\n",
    "    // Pointer to stack allocated in GPU memory\n",
    "    Stack* d_stack;\n",
    "\n",
    "    // Pointer to output array in GPU memory\n",
    "    int* d_output;\n",
    "\n",
    "    // Allocate memory for stack on GPU\n",
    "    cudaMalloc(&d_stack, sizeof(Stack));\n",
    "\n",
    "    // Allocate memory for output array on GPU\n",
    "    cudaMalloc(&d_output, THREADS * sizeof(int));\n",
    "\n",
    "    // Host-side stack structure\n",
    "    Stack h_stack;\n",
    "\n",
    "    // Initialize stack pointer to zero (empty stack)\n",
    "    h_stack.top = 0;\n",
    "\n",
    "    // Copy initialized stack from host to device\n",
    "    cudaMemcpy(d_stack, &h_stack, sizeof(Stack), cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Launch kernel to push values in parallel\n",
    "    push_kernel<<<1, THREADS>>>(d_stack);\n",
    "\n",
    "    // Wait until push kernel finishes\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    // Launch kernel to pop values in parallel\n",
    "    pop_kernel<<<1, THREADS>>>(d_stack, d_output);\n",
    "\n",
    "    // Wait until pop kernel finishes\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    // Host-side array to store popped values\n",
    "    int h_output[THREADS];\n",
    "\n",
    "    // Copy popped values from device to host\n",
    "    cudaMemcpy(h_output, d_output, THREADS * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // Counter for successful pop operations\n",
    "    int success = 0;\n",
    "\n",
    "    // Check correctness of pops\n",
    "    for (int i = 0; i < THREADS; i++) {\n",
    "        if (h_output[i] != -1)\n",
    "            success++;\n",
    "    }\n",
    "\n",
    "    // Print results\n",
    "    std::cout << \"Successful pops: \" << success << std::endl;\n",
    "    std::cout << \"Expected (<= capacity): \" << STACK_CAPACITY << std::endl;\n",
    "\n",
    "    // Free GPU memory\n",
    "    cudaFree(d_stack);\n",
    "    cudaFree(d_output);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_9-pCjkvlnBl"
   },
   "outputs": [],
   "source": [
    "!nvcc stack_cuda.cu -o stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29je3ucNlqji",
    "outputId": "01ece166-e3fb-4a62-fff1-1ffbf82d9a27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful pops: 256\n",
      "Expected (<= capacity): 1024\n"
     ]
    }
   ],
   "source": [
    "!./stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wStxmIeImh6o",
    "outputId": "f7eb02a9-2757-4877-c3d6-5dfc80f3d3e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing queue_vs_stack.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile queue_vs_stack.cu\n",
    "\n",
    "// CUDA runtime API (memory management, kernel launches, atomics)\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Standard C++ input/output\n",
    "#include <iostream>\n",
    "\n",
    "// Fixed capacity for both stack and queue\n",
    "#define CAPACITY 1024\n",
    "\n",
    "// Number of CUDA threads\n",
    "#define THREADS 256\n",
    "\n",
    "// ======================\n",
    "// STACK STRUCTURE\n",
    "// ======================\n",
    "\n",
    "// Stack stored in global memory\n",
    "struct Stack {\n",
    "    int data[CAPACITY];   // Stack storage\n",
    "    int top;              // Stack pointer\n",
    "};\n",
    "\n",
    "// ======================\n",
    "// QUEUE STRUCTURE\n",
    "// ======================\n",
    "\n",
    "// Queue stored in global memory (circular buffer)\n",
    "struct Queue {\n",
    "    int data[CAPACITY];   // Queue storage\n",
    "    int head;             // Index for dequeue\n",
    "    int tail;             // Index for enqueue\n",
    "    int size;             // Current number of elements\n",
    "};\n",
    "\n",
    "// ======================\n",
    "// STACK PUSH (device)\n",
    "// ======================\n",
    "\n",
    "// Push value onto stack\n",
    "__device__ bool stack_push(Stack* s, int value) {\n",
    "\n",
    "    // Atomically increment stack pointer\n",
    "    int idx = atomicAdd(&s->top, 1);\n",
    "\n",
    "    // Check overflow\n",
    "    if (idx >= CAPACITY) {\n",
    "        // Roll back if full\n",
    "        atomicSub(&s->top, 1);\n",
    "        return false;\n",
    "    }\n",
    "\n",
    "    // Store value\n",
    "    s->data[idx] = value;\n",
    "    return true;\n",
    "}\n",
    "\n",
    "// ======================\n",
    "// STACK POP (device)\n",
    "// ======================\n",
    "\n",
    "// Pop value from stack\n",
    "__device__ bool stack_pop(Stack* s, int* value) {\n",
    "\n",
    "    // Atomically decrement stack pointer\n",
    "    int idx = atomicSub(&s->top, 1) - 1;\n",
    "\n",
    "    // Check underflow\n",
    "    if (idx < 0) {\n",
    "        // Roll back if empty\n",
    "        atomicAdd(&s->top, 1);\n",
    "        return false;\n",
    "    }\n",
    "\n",
    "    // Load value\n",
    "    *value = s->data[idx];\n",
    "    return true;\n",
    "}\n",
    "\n",
    "// ======================\n",
    "// QUEUE ENQUEUE (device)\n",
    "// ======================\n",
    "\n",
    "// Add value to queue\n",
    "__device__ bool queue_enqueue(Queue* q, int value) {\n",
    "\n",
    "    // Atomically reserve a position\n",
    "    int pos = atomicAdd(&q->tail, 1);\n",
    "\n",
    "    // Check if queue is full\n",
    "    if (atomicAdd(&q->size, 1) >= CAPACITY) {\n",
    "        // Roll back changes\n",
    "        atomicSub(&q->tail, 1);\n",
    "        atomicSub(&q->size, 1);\n",
    "        return false;\n",
    "    }\n",
    "\n",
    "    // Store value using circular indexing\n",
    "    q->data[pos % CAPACITY] = value;\n",
    "    return true;\n",
    "}\n",
    "\n",
    "// ======================\n",
    "// QUEUE DEQUEUE (device)\n",
    "// ======================\n",
    "\n",
    "// Remove value from queue\n",
    "__device__ bool queue_dequeue(Queue* q, int* value) {\n",
    "\n",
    "    // Check if queue is empty\n",
    "    if (atomicSub(&q->size, 1) <= 0) {\n",
    "        // Roll back if empty\n",
    "        atomicAdd(&q->size, 1);\n",
    "        return false;\n",
    "    }\n",
    "\n",
    "    // Atomically reserve dequeue position\n",
    "    int pos = atomicAdd(&q->head, 1);\n",
    "\n",
    "    // Load value using circular indexing\n",
    "    *value = q->data[pos % CAPACITY];\n",
    "    return true;\n",
    "}\n",
    "\n",
    "// ======================\n",
    "// STACK KERNELS\n",
    "// ======================\n",
    "\n",
    "// Parallel stack push\n",
    "__global__ void stack_push_kernel(Stack* s) {\n",
    "\n",
    "    // Global thread index\n",
    "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "\n",
    "    // Each thread pushes its ID\n",
    "    stack_push(s, tid);\n",
    "}\n",
    "\n",
    "// Parallel stack pop\n",
    "__global__ void stack_pop_kernel(Stack* s, int* out) {\n",
    "\n",
    "    // Global thread index\n",
    "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "\n",
    "    int value;\n",
    "\n",
    "    // Attempt pop\n",
    "    if (stack_pop(s, &value))\n",
    "        out[tid] = value;\n",
    "    else\n",
    "        out[tid] = -1;\n",
    "}\n",
    "\n",
    "// ======================\n",
    "// QUEUE KERNELS\n",
    "// ======================\n",
    "\n",
    "// Parallel enqueue\n",
    "__global__ void queue_enqueue_kernel(Queue* q) {\n",
    "\n",
    "    // Global thread index\n",
    "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "\n",
    "    // Each thread enqueues its ID\n",
    "    queue_enqueue(q, tid);\n",
    "}\n",
    "\n",
    "// Parallel dequeue\n",
    "__global__ void queue_dequeue_kernel(Queue* q, int* out) {\n",
    "\n",
    "    // Global thread index\n",
    "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "\n",
    "    int value;\n",
    "\n",
    "    // Attempt dequeue\n",
    "    if (queue_dequeue(q, &value))\n",
    "        out[tid] = value;\n",
    "    else\n",
    "        out[tid] = -1;\n",
    "}\n",
    "\n",
    "// ======================\n",
    "// MAIN FUNCTION\n",
    "// ======================\n",
    "\n",
    "int main() {\n",
    "\n",
    "    // Device pointers\n",
    "    Stack* d_stack;\n",
    "    Queue* d_queue;\n",
    "    int* d_output;\n",
    "\n",
    "    // Allocate GPU memory\n",
    "    cudaMalloc(&d_stack, sizeof(Stack));\n",
    "    cudaMalloc(&d_queue, sizeof(Queue));\n",
    "    cudaMalloc(&d_output, THREADS * sizeof(int));\n",
    "\n",
    "    // Host stack initialization\n",
    "    Stack h_stack;\n",
    "    h_stack.top = 0;\n",
    "\n",
    "    // Host queue initialization\n",
    "    Queue h_queue;\n",
    "    h_queue.head = 0;\n",
    "    h_queue.tail = 0;\n",
    "    h_queue.size = 0;\n",
    "\n",
    "    // Copy to device\n",
    "    cudaMemcpy(d_stack, &h_stack, sizeof(Stack), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_queue, &h_queue, sizeof(Queue), cudaMemcpyHostToDevice);\n",
    "\n",
    "    // CUDA events for timing\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "\n",
    "    // ======================\n",
    "    // STACK TIMING\n",
    "    // ======================\n",
    "\n",
    "    cudaEventRecord(start);\n",
    "\n",
    "    stack_push_kernel<<<1, THREADS>>>(d_stack);\n",
    "    stack_pop_kernel<<<1, THREADS>>>(d_stack, d_output);\n",
    "\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "\n",
    "    float stack_time;\n",
    "    cudaEventElapsedTime(&stack_time, start, stop);\n",
    "\n",
    "    // ======================\n",
    "    // QUEUE TIMING\n",
    "    // ======================\n",
    "\n",
    "    cudaEventRecord(start);\n",
    "\n",
    "    queue_enqueue_kernel<<<1, THREADS>>>(d_queue);\n",
    "    queue_dequeue_kernel<<<1, THREADS>>>(d_queue, d_output);\n",
    "\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "\n",
    "    float queue_time;\n",
    "    cudaEventElapsedTime(&queue_time, start, stop);\n",
    "\n",
    "    // ======================\n",
    "    // OUTPUT RESULTS\n",
    "    // ======================\n",
    "\n",
    "    std::cout << \"Stack execution time: \" << stack_time << \" ms\" << std::endl;\n",
    "    std::cout << \"Queue execution time: \" << queue_time << \" ms\" << std::endl;\n",
    "\n",
    "    // Free GPU memory\n",
    "    cudaFree(d_stack);\n",
    "    cudaFree(d_queue);\n",
    "    cudaFree(d_output);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JVPlqSTtmiuc"
   },
   "outputs": [],
   "source": [
    "!nvcc queue_vs_stack.cu -o queue_vs_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hM3MdyTVmpQc",
    "outputId": "a795582f-e3ab-45d2-b148-ba258434d775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack execution time: 7.5271 ms\n",
      "Queue execution time: 0.002048 ms\n"
     ]
    }
   ],
   "source": [
    "!./queue_vs_stack"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
