{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Задание 1. Анализ производительности CPU-параллельной программы (OpenMP)"
      ],
      "metadata": {
        "id": "XHLZ5d9IB_AP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YgNnHEXB9nA",
        "outputId": "6eca6240-51ae-454c-d4eb-51d2bb2287f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task1.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile task1.cpp\n",
        "\n",
        "#include <iostream>   // ввод-вывод\n",
        "#include <vector>     // контейнер std::vector\n",
        "#include <omp.h>      // OpenMP\n",
        "\n",
        "int main() {\n",
        "\n",
        "    // Размер массива\n",
        "    const int N = 10'000'000;\n",
        "\n",
        "    // Массив данных, инициализированный значениями 1.0\n",
        "    std::vector<double> data(N, 1.0);\n",
        "\n",
        "    // Переменные для измерения времени выполнения\n",
        "    double start, end;\n",
        "\n",
        "    // ==============================\n",
        "    // Начало измерения времени\n",
        "    // ==============================\n",
        "    start = omp_get_wtime();\n",
        "\n",
        "    // ------------------------------\n",
        "    // Параллельное вычисление суммы\n",
        "    // ------------------------------\n",
        "    double sum = 0.0;\n",
        "\n",
        "    // reduction(+:sum) — каждая нить считает свою локальную сумму,\n",
        "    // затем все частичные результаты суммируются\n",
        "    #pragma omp parallel for reduction(+:sum)\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        sum += data[i];\n",
        "    }\n",
        "\n",
        "    // Вычисление среднего значения\n",
        "    double mean = sum / N;\n",
        "\n",
        "    // -------------------------------------\n",
        "    // Параллельное вычисление дисперсии\n",
        "    // -------------------------------------\n",
        "    double variance = 0.0;\n",
        "\n",
        "    // Аналогично сумме используется редукция\n",
        "    #pragma omp parallel for reduction(+:variance)\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        double diff = data[i] - mean;\n",
        "        variance += diff * diff;\n",
        "    }\n",
        "\n",
        "    // Нормализация дисперсии\n",
        "    variance /= N;\n",
        "\n",
        "    // ==============================\n",
        "    // Окончание измерения времени\n",
        "    // ==============================\n",
        "    end = omp_get_wtime();\n",
        "\n",
        "    // Вывод времени выполнения и результатов вычислений\n",
        "    std::cout << \"Time: \" << end - start << \" seconds\\n\";\n",
        "    std::cout << \"Mean: \" << mean << \"\\n\";\n",
        "    std::cout << \"Variance: \" << variance << \"\\n\";\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ -fopenmp task1.cpp -o task1\n",
        "!OMP_NUM_THREADS=1 ./task1\n",
        "!OMP_NUM_THREADS=2 ./task1\n",
        "!OMP_NUM_THREADS=4 ./task1\n",
        "!OMP_NUM_THREADS=8 ./task1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62jLLWfyC9-e",
        "outputId": "0fb60d46-6fae-4f2c-cbca-554b1cc0224b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 0.0864647 seconds\n",
            "Mean: 1\n",
            "Variance: 0\n",
            "Time: 0.07834 seconds\n",
            "Mean: 1\n",
            "Variance: 0\n",
            "Time: 0.084849 seconds\n",
            "Mean: 1\n",
            "Variance: 0\n",
            "Time: 0.0859424 seconds\n",
            "Mean: 1\n",
            "Variance: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 2. Оптимизация доступа к памяти на GPU (CUDA)"
      ],
      "metadata": {
        "id": "KGekmLwcDNxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task2.cu\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "\n",
        "#define N (1 << 24)        // ~16 млн элементов\n",
        "#define THREADS 256        // число потоков в блоке\n",
        "#define STRIDE 32          // шаг для некоалесцированного доступа\n",
        "\n",
        "// =====================================================\n",
        "// Ядро 1: Коалесцированный доступ к глобальной памяти\n",
        "// Каждый поток работает с соседним элементом массива\n",
        "// =====================================================\n",
        "__global__ void coalesced_kernel(float* data, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n) {\n",
        "        data[idx] *= 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "// =====================================================\n",
        "// Ядро 2: Некоалесцированный доступ к памяти\n",
        "// Потоки обращаются к данным с большим шагом (stride)\n",
        "// =====================================================\n",
        "__global__ void noncoalesced_kernel(float* data, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int access_idx = idx * STRIDE;\n",
        "    if (access_idx < n) {\n",
        "        data[access_idx] *= 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "// =====================================================\n",
        "// Ядро 3: Использование разделяемой памяти\n",
        "// Данные сначала копируются в shared memory,\n",
        "// затем обрабатываются и записываются обратно\n",
        "// =====================================================\n",
        "__global__ void shared_memory_kernel(float* data, int n) {\n",
        "    __shared__ float buffer[THREADS];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int idx = blockIdx.x * blockDim.x + tid;\n",
        "\n",
        "    // Загрузка из глобальной памяти в shared memory\n",
        "    if (idx < n) {\n",
        "        buffer[tid] = data[idx];\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // Обработка данных в shared memory\n",
        "    if (idx < n) {\n",
        "        buffer[tid] *= 2.0f;\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // Запись обратно в глобальную память\n",
        "    if (idx < n) {\n",
        "        data[idx] = buffer[tid];\n",
        "    }\n",
        "}\n",
        "\n",
        "// =====================================================\n",
        "// Функция измерения времени выполнения ядра\n",
        "// =====================================================\n",
        "void measure_kernel(\n",
        "    void (*kernel)(float*, int),\n",
        "    float* d_data,\n",
        "    int n,\n",
        "    const char* label\n",
        ") {\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    int blocks = (n + THREADS - 1) / THREADS;\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "    kernel<<<blocks, THREADS>>>(d_data, n);\n",
        "    cudaEventRecord(stop);\n",
        "\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float time_ms = 0.0f;\n",
        "    cudaEventElapsedTime(&time_ms, start, stop);\n",
        "\n",
        "    std::cout << label << \" time: \" << time_ms << \" ms\\n\";\n",
        "\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "}\n",
        "\n",
        "// =====================================================\n",
        "// Главная функция\n",
        "// =====================================================\n",
        "int main() {\n",
        "\n",
        "    size_t size = N * sizeof(float);\n",
        "\n",
        "    // Выделение памяти на CPU\n",
        "    std::vector<float> h_data(N, 1.0f);\n",
        "\n",
        "    // Выделение памяти на GPU\n",
        "    float* d_data;\n",
        "    cudaMalloc(&d_data, size);\n",
        "\n",
        "    // Копирование данных на GPU\n",
        "    cudaMemcpy(d_data, h_data.data(), size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    std::cout << \"CUDA memory access analysis\\n\";\n",
        "\n",
        "    // ===== Коалесцированный доступ =====\n",
        "    measure_kernel(coalesced_kernel, d_data, N,\n",
        "                   \"Coalesced access\");\n",
        "\n",
        "    // ===== Некоалесцированный доступ =====\n",
        "    measure_kernel(noncoalesced_kernel, d_data, N,\n",
        "                   \"Non-coalesced access\");\n",
        "\n",
        "    // ===== Shared memory =====\n",
        "    measure_kernel(shared_memory_kernel, d_data, N,\n",
        "                   \"Shared memory\");\n",
        "\n",
        "    // Освобождение памяти GPU\n",
        "    cudaFree(d_data);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-eHU5k5DSc8",
        "outputId": "3b2bec7c-50f0-4eaa-e395-2216a36e42da"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing task2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -gencode=arch=compute_75,code=sm_75 task2.cu -o task2\n",
        "!./task2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IABcfVwrDS1s",
        "outputId": "34a98960-f54e-4497-e35a-33c8acdf3927"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA memory access analysis\n",
            "Coalesced access time: 0.705184 ms\n",
            "Non-coalesced access time: 0.655232 ms\n",
            "Shared memory time: 0.698336 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 3. Профилирование гибридного приложения CPU + GPU"
      ],
      "metadata": {
        "id": "BoIA_Ut8DTI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hybrid_async.cu\n",
        "\n",
        "#include <iostream>        // ввод-вывод\n",
        "#include <vector>          // контейнер std::vector\n",
        "#include <omp.h>           // OpenMP\n",
        "#include <cuda_runtime.h>  // CUDA Runtime API\n",
        "\n",
        "#define N 10000000         // размер массива\n",
        "#define THREADS 256        // количество потоков в CUDA-блоке\n",
        "\n",
        "// =====================================================\n",
        "// CUDA-ядро для обработки части массива на GPU\n",
        "// Каждый поток обрабатывает один элемент\n",
        "// =====================================================\n",
        "__global__ void gpu_kernel(float* data, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n) {\n",
        "        data[idx] *= 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "\n",
        "    // =====================================================\n",
        "    // Подготовка данных на CPU\n",
        "    // =====================================================\n",
        "    // Инициализация массива значениями 1.0\n",
        "    std::vector<float> h_data(N, 1.0f);\n",
        "\n",
        "    // Делим массив на две части:\n",
        "    // первая половина — CPU, вторая — GPU\n",
        "    int half = N / 2;\n",
        "    size_t size_half = half * sizeof(float);\n",
        "\n",
        "    // =====================================================\n",
        "    // Выделение памяти на GPU\n",
        "    // =====================================================\n",
        "    float* d_data;\n",
        "    cudaMalloc(&d_data, size_half);\n",
        "\n",
        "    // Создание CUDA stream для асинхронных операций\n",
        "    cudaStream_t stream;\n",
        "    cudaStreamCreate(&stream);\n",
        "\n",
        "    // =====================================================\n",
        "    // Начало измерения времени\n",
        "    // =====================================================\n",
        "    double start = omp_get_wtime();\n",
        "\n",
        "    // =====================================================\n",
        "    // Асинхронная передача данных CPU → GPU\n",
        "    // Копируется вторая половина массива\n",
        "    // =====================================================\n",
        "    cudaMemcpyAsync(\n",
        "        d_data,                     // память GPU\n",
        "        h_data.data() + half,       // вторая половина массива CPU\n",
        "        size_half,\n",
        "        cudaMemcpyHostToDevice,\n",
        "        stream                      // асинхронный stream\n",
        "    );\n",
        "\n",
        "    // =====================================================\n",
        "    // Параллельная обработка CPU и GPU\n",
        "    // =====================================================\n",
        "    #pragma omp parallel sections\n",
        "    {\n",
        "        // -----------------------------\n",
        "        // CPU-часть вычислений\n",
        "        // -----------------------------\n",
        "        #pragma omp section\n",
        "        {\n",
        "            for (int i = 0; i < half; i++) {\n",
        "                h_data[i] *= 2.0f;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // -----------------------------\n",
        "        // GPU-часть вычислений\n",
        "        // -----------------------------\n",
        "        #pragma omp section\n",
        "        {\n",
        "            int blocks = (half + THREADS - 1) / THREADS;\n",
        "            gpu_kernel<<<blocks, THREADS, 0, stream>>>(d_data, half);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // =====================================================\n",
        "    // Асинхронная передача данных GPU → CPU\n",
        "    // =====================================================\n",
        "    cudaMemcpyAsync(\n",
        "        h_data.data() + half,   // куда копируем\n",
        "        d_data,                 // откуда копируем\n",
        "        size_half,\n",
        "        cudaMemcpyDeviceToHost,\n",
        "        stream\n",
        "    );\n",
        "\n",
        "    // Ожидание завершения всех операций в stream\n",
        "    cudaStreamSynchronize(stream);\n",
        "\n",
        "    // =====================================================\n",
        "    // Окончание измерения времени\n",
        "    // =====================================================\n",
        "    double end = omp_get_wtime();\n",
        "\n",
        "    // =====================================================\n",
        "    // Освобождение ресурсов\n",
        "    // =====================================================\n",
        "    cudaFree(d_data);\n",
        "    cudaStreamDestroy(stream);\n",
        "\n",
        "    // Вывод времени выполнения гибридного алгоритма\n",
        "    std::cout << \"Hybrid async time: \" << end - start << \" seconds\\n\";\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fZ21Fm4DVbF",
        "outputId": "b70989ea-4cfc-4b09-c78a-7af6ca7fe734"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hybrid_async.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -Xcompiler -fopenmp hybrid_async.cu -o hybrid_async\n",
        "!./hybrid_async"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq5m0bk2DVnD",
        "outputId": "233641a6-d874-4b73-d79b-00fc95e2de74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid async time: 0.0836766 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 4. Анализ масштабируемости распределённой программы (MPI)"
      ],
      "metadata": {
        "id": "Yb_uEjT7DWwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task4.cpp\n",
        "\n",
        "#include <mpi.h>        // MPI библиотека\n",
        "#include <iostream>    // ввод-вывод\n",
        "#include <vector>      // контейнер std::vector\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "\n",
        "    // Инициализация MPI\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank, size;\n",
        "\n",
        "    // Номер текущего процесса\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "\n",
        "    // Общее количество процессов\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    // =================================================\n",
        "    // Параметры задачи\n",
        "    // =================================================\n",
        "\n",
        "    // Размер массива на ОДИН процесс (используется для weak scaling)\n",
        "    const long long LOCAL_N = 10'000'000;\n",
        "\n",
        "    // Общий размер массива для strong scaling\n",
        "    const long long GLOBAL_N = LOCAL_N;\n",
        "\n",
        "    // -------------------------------------------------\n",
        "    // Выбор режима масштабирования\n",
        "    // true  — weak scaling  (нагрузка на процесс фиксирована)\n",
        "    // false — strong scaling (общий объём данных фиксирован)\n",
        "    // -------------------------------------------------\n",
        "    bool weak_scaling = true;\n",
        "\n",
        "    // Определяем размер локального массива\n",
        "    long long local_n;\n",
        "    if (weak_scaling) {\n",
        "        // Weak scaling: каждый процесс обрабатывает одинаковый объём данных\n",
        "        local_n = LOCAL_N;\n",
        "    } else {\n",
        "        // Strong scaling: общий объём данных делится между процессами\n",
        "        local_n = GLOBAL_N / size;\n",
        "    }\n",
        "\n",
        "    // =================================================\n",
        "    // Локальные данные\n",
        "    // =================================================\n",
        "    // Каждый процесс создаёт свой локальный массив\n",
        "    std::vector<double> local_data(local_n, 1.0);\n",
        "\n",
        "    // =================================================\n",
        "    // Начало измерения времени\n",
        "    // =================================================\n",
        "    double start = MPI_Wtime();\n",
        "\n",
        "    // =================================================\n",
        "    // Локальные вычисления\n",
        "    // =================================================\n",
        "    // Каждый процесс вычисляет сумму своей части массива\n",
        "    double local_sum = 0.0;\n",
        "    for (long long i = 0; i < local_n; i++) {\n",
        "        local_sum += local_data[i];\n",
        "    }\n",
        "\n",
        "    // Переменная для глобальной суммы\n",
        "    double global_sum = 0.0;\n",
        "\n",
        "    // =================================================\n",
        "    // Коллективная операция MPI_Reduce\n",
        "    // =================================================\n",
        "    // Локальные суммы объединяются в одну глобальную\n",
        "    // Результат доступен только на процессе rank = 0\n",
        "    MPI_Reduce(\n",
        "        &local_sum,        // локальное значение\n",
        "        &global_sum,       // глобальный результат (только root)\n",
        "        1,                 // количество элементов\n",
        "        MPI_DOUBLE,        // тип данных\n",
        "        MPI_SUM,           // операция суммирования\n",
        "        0,                 // root-процесс\n",
        "        MPI_COMM_WORLD\n",
        "    );\n",
        "\n",
        "    // =================================================\n",
        "    // Окончание измерения времени\n",
        "    // =================================================\n",
        "    double end = MPI_Wtime();\n",
        "\n",
        "    // =================================================\n",
        "    // Вывод результатов (только root)\n",
        "    // =================================================\n",
        "    if (rank == 0) {\n",
        "        std::cout << \"Processes: \" << size << \"\\n\";\n",
        "        std::cout << \"Execution time: \" << end - start << \" seconds\\n\";\n",
        "        std::cout << \"Global sum: \" << global_sum << \"\\n\\n\";\n",
        "    }\n",
        "\n",
        "    // Завершение работы MPI\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNIml_MSI3iA",
        "outputId": "30caef10-a5b5-4fcb-8609-d012aac291a1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task4.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpic++ task4.cpp -o task4\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 1 ./task4\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./task4\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 4 ./task4\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 8 ./task4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY5nKR06I30b",
        "outputId": "92dd3ce2-085a-4d70-a942-2cfde4432f49"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processes: 1\n",
            "Execution time: 0.0373409 seconds\n",
            "Global sum: 1e+07\n",
            "\n",
            "Processes: 2\n",
            "Execution time: 0.0711676 seconds\n",
            "Global sum: 2e+07\n",
            "\n",
            "Processes: 4\n",
            "Execution time: 0.124943 seconds\n",
            "Global sum: 4e+07\n",
            "\n",
            "Processes: 8\n",
            "Execution time: 0.248887 seconds\n",
            "Global sum: 8e+07\n",
            "\n"
          ]
        }
      ]
    }
  ]
}