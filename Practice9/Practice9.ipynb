{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Задание 1: Распределённое вычисление среднего значения и стандартного\n",
        "отклонения"
      ],
      "metadata": {
        "id": "r_Ko2Ki6yogZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fnjq7xsyWFL",
        "outputId": "1442dbe2-c082-499b-efef-6ea0d004189a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.cpp\n",
        "\n",
        "#include <mpi.h>      // библиотека MPI\n",
        "#include <iostream>  // ввод-вывод\n",
        "#include <vector>    // std::vector\n",
        "#include <cmath>     // sqrt\n",
        "#include <cstdlib>   // rand, RAND_MAX\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "\n",
        "    // Инициализация среды MPI\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank, size;\n",
        "\n",
        "    // Получение номера текущего процесса\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "\n",
        "    // Получение общего количества процессов\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    // Общий размер массива\n",
        "    const int N = 1'000'000;\n",
        "\n",
        "    // Полный массив данных (используется только на процессе rank = 0)\n",
        "    std::vector<double> data;\n",
        "\n",
        "    // Массивы для MPI_Scatterv:\n",
        "    // counts — количество элементов для каждого процесса\n",
        "    // displs — смещения начала данных для каждого процесса\n",
        "    std::vector<int> counts(size);\n",
        "    std::vector<int> displs(size);\n",
        "\n",
        "    // Процесс с rank = 0 инициализирует массив случайных чисел\n",
        "    if (rank == 0) {\n",
        "        data.resize(N);\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            data[i] = static_cast<double>(rand()) / RAND_MAX;\n",
        "        }\n",
        "\n",
        "        // Вычисление количества элементов для каждого процесса\n",
        "        int base = N / size; // базовое количество\n",
        "        int rem  = N % size; // остаток\n",
        "\n",
        "        for (int i = 0; i < size; i++) {\n",
        "            // Первые rem процессов получают на один элемент больше\n",
        "            counts[i] = base + (i < rem ? 1 : 0);\n",
        "\n",
        "            // Вычисление смещений\n",
        "            displs[i] = (i == 0) ? 0 : displs[i - 1] + counts[i - 1];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Рассылка количества элементов каждому процессу\n",
        "    int local_n;\n",
        "    MPI_Scatter(counts.data(), 1, MPI_INT,\n",
        "                &local_n, 1, MPI_INT,\n",
        "                0, MPI_COMM_WORLD);\n",
        "\n",
        "    // Локальный массив данных каждого процесса\n",
        "    std::vector<double> local_data(local_n);\n",
        "\n",
        "    // Распределение частей массива между процессами\n",
        "    MPI_Scatterv(data.data(), counts.data(), displs.data(), MPI_DOUBLE,\n",
        "                 local_data.data(), local_n, MPI_DOUBLE,\n",
        "                 0, MPI_COMM_WORLD);\n",
        "\n",
        "    // Локальные суммы\n",
        "    double local_sum = 0.0;\n",
        "    double local_sq_sum = 0.0;\n",
        "\n",
        "    // Вычисление суммы и суммы квадратов элементов локального массива\n",
        "    for (int i = 0; i < local_n; i++) {\n",
        "        local_sum += local_data[i];\n",
        "        local_sq_sum += local_data[i] * local_data[i];\n",
        "    }\n",
        "\n",
        "    // Переменные для глобальных результатов\n",
        "    double global_sum = 0.0;\n",
        "    double global_sq_sum = 0.0;\n",
        "\n",
        "    // Сбор локальных сумм на процессе rank = 0\n",
        "    MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n",
        "    MPI_Reduce(&local_sq_sum, &global_sq_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    // Вычисление среднего значения и стандартного отклонения\n",
        "    if (rank == 0) {\n",
        "        double mean = global_sum / N;\n",
        "\n",
        "        // Дисперсия по формуле Var = E[x^2] - (E[x])^2\n",
        "        double variance = (global_sq_sum / N) - (mean * mean);\n",
        "\n",
        "        // Стандартное отклонение\n",
        "        double std_dev = std::sqrt(variance);\n",
        "\n",
        "        // Вывод результатов\n",
        "        std::cout << \"Mean value: \" << mean << std::endl;\n",
        "        std::cout << \"Standard deviation: \" << std_dev << std::endl;\n",
        "    }\n",
        "\n",
        "    // Завершение работы MPI\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpic++ -o main main.cpp\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 4 ./main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU4oWQrPzBp_",
        "outputId": "f57b3c41-ca8d-4c2c-b767-bbf0421a8cd5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean value: 0.500007\n",
            "Standard deviation: 0.28862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 2: Распределённое решение системы линейных уравнений методом\n",
        "Гаусса"
      ],
      "metadata": {
        "id": "LEzf38BNzBB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task2.cpp\n",
        "\n",
        "#include <mpi.h>        // MPI библиотека\n",
        "#include <iostream>    // ввод-вывод\n",
        "#include <vector>      // контейнер std::vector\n",
        "#include <cmath>       // математические функции\n",
        "#include <algorithm>   // std::min\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "\n",
        "    // Инициализация среды MPI\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank, size;\n",
        "\n",
        "    // Получение номера текущего процесса\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "\n",
        "    // Получение общего количества процессов\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    // ===== Параметры задачи =====\n",
        "    // Размер системы линейных уравнений (NxN)\n",
        "    int N = 4; // можно изменять или передавать через аргументы\n",
        "\n",
        "    // ===== Полная матрица коэффициентов A и вектор правых частей b =====\n",
        "    // Используются только на процессе rank = 0\n",
        "    std::vector<double> A;\n",
        "    std::vector<double> b;\n",
        "\n",
        "    // Инициализация матрицы и вектора на процессе rank = 0\n",
        "    if (rank == 0) {\n",
        "        A.resize(N * N);\n",
        "        b.resize(N);\n",
        "\n",
        "        // Заполнение матрицы и вектора значениями\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            for (int j = 0; j < N; j++) {\n",
        "                // Диагональные элементы больше остальных\n",
        "                A[i * N + j] = (i == j) ? 4.0 : 1.0;\n",
        "            }\n",
        "            b[i] = N + 1;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // ===== Распределение строк матрицы =====\n",
        "    // Базовое количество строк на процесс\n",
        "    int base = N / size;\n",
        "\n",
        "    // Остаток строк, если N не делится на количество процессов\n",
        "    int rem  = N % size;\n",
        "\n",
        "    // Количество строк, обрабатываемых текущим процессом\n",
        "    int local_rows = base + (rank < rem ? 1 : 0);\n",
        "\n",
        "    // Смещение глобальных строк для текущего процесса\n",
        "    int offset = rank * base + std::min(rank, rem);\n",
        "\n",
        "    // ===== Локальные части матрицы и вектора =====\n",
        "    std::vector<double> local_A(local_rows * N);\n",
        "    std::vector<double> local_b(local_rows);\n",
        "\n",
        "    // ===== Параметры для MPI_Scatterv =====\n",
        "    std::vector<int> sendcounts_A, displs_A;\n",
        "    std::vector<int> sendcounts_b, displs_b;\n",
        "\n",
        "    if (rank == 0) {\n",
        "        sendcounts_A.resize(size);\n",
        "        displs_A.resize(size);\n",
        "        sendcounts_b.resize(size);\n",
        "        displs_b.resize(size);\n",
        "\n",
        "        int dispA = 0;\n",
        "        int dispb = 0;\n",
        "\n",
        "        // Подсчёт количества элементов и смещений\n",
        "        for (int i = 0; i < size; i++) {\n",
        "            int rows = base + (i < rem ? 1 : 0);\n",
        "\n",
        "            // Для матрицы A\n",
        "            sendcounts_A[i] = rows * N;\n",
        "            displs_A[i] = dispA;\n",
        "            dispA += rows * N;\n",
        "\n",
        "            // Для вектора b\n",
        "            sendcounts_b[i] = rows;\n",
        "            displs_b[i] = dispb;\n",
        "            dispb += rows;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // ===== Рассылка матрицы A между процессами =====\n",
        "    MPI_Scatterv(A.data(), sendcounts_A.data(), displs_A.data(), MPI_DOUBLE,\n",
        "                 local_A.data(), local_rows * N, MPI_DOUBLE,\n",
        "                 0, MPI_COMM_WORLD);\n",
        "\n",
        "    // ===== Рассылка вектора b между процессами =====\n",
        "    MPI_Scatterv(b.data(), sendcounts_b.data(), displs_b.data(), MPI_DOUBLE,\n",
        "                 local_b.data(), local_rows, MPI_DOUBLE,\n",
        "                 0, MPI_COMM_WORLD);\n",
        "\n",
        "    // ===== Буферы для ведущей строки =====\n",
        "    std::vector<double> pivot_row(N);\n",
        "    double pivot_b = 0.0;\n",
        "\n",
        "    // ===== Прямой ход метода Гаусса =====\n",
        "    for (int k = 0; k < N; k++) {\n",
        "\n",
        "        // Определение процесса-владельца ведущей строки k\n",
        "        int owner;\n",
        "        if (k < (base + 1) * rem)\n",
        "            owner = k / (base + 1);\n",
        "        else\n",
        "            owner = rem + (k - (base + 1) * rem) / base;\n",
        "\n",
        "        // Процесс-владелец копирует ведущую строку\n",
        "        if (rank == owner) {\n",
        "            int local_k = k - offset;\n",
        "            for (int j = 0; j < N; j++) {\n",
        "                pivot_row[j] = local_A[local_k * N + j];\n",
        "            }\n",
        "            pivot_b = local_b[local_k];\n",
        "        }\n",
        "\n",
        "        // Передача ведущей строки всем процессам\n",
        "        MPI_Bcast(pivot_row.data(), N, MPI_DOUBLE, owner, MPI_COMM_WORLD);\n",
        "        MPI_Bcast(&pivot_b, 1, MPI_DOUBLE, owner, MPI_COMM_WORLD);\n",
        "\n",
        "        // Обнуление элементов ниже ведущего в локальных строках\n",
        "        for (int i = 0; i < local_rows; i++) {\n",
        "            int global_i = offset + i;\n",
        "            if (global_i > k) {\n",
        "                double factor = local_A[i * N + k] / pivot_row[k];\n",
        "                for (int j = k; j < N; j++) {\n",
        "                    local_A[i * N + j] -= factor * pivot_row[j];\n",
        "                }\n",
        "                local_b[i] -= factor * pivot_b;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // ===== Сбор преобразованной матрицы и вектора =====\n",
        "    MPI_Gatherv(local_A.data(), local_rows * N, MPI_DOUBLE,\n",
        "                A.data(), sendcounts_A.data(), displs_A.data(), MPI_DOUBLE,\n",
        "                0, MPI_COMM_WORLD);\n",
        "\n",
        "    MPI_Gatherv(local_b.data(), local_rows, MPI_DOUBLE,\n",
        "                b.data(), sendcounts_b.data(), displs_b.data(), MPI_DOUBLE,\n",
        "                0, MPI_COMM_WORLD);\n",
        "\n",
        "    // ===== Обратный ход метода Гаусса (только rank = 0) =====\n",
        "    if (rank == 0) {\n",
        "        std::vector<double> x(N);\n",
        "\n",
        "        for (int i = N - 1; i >= 0; i--) {\n",
        "            x[i] = b[i];\n",
        "            for (int j = i + 1; j < N; j++) {\n",
        "                x[i] -= A[i * N + j] * x[j];\n",
        "            }\n",
        "            x[i] /= A[i * N + i];\n",
        "        }\n",
        "\n",
        "        // Вывод решения системы\n",
        "        std::cout << \"Solution vector x:\\n\";\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            std::cout << \"x[\" << i << \"] = \" << x[i] << std::endl;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Завершение работы MPI\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzDY5x6QzlaC",
        "outputId": "eea3ed29-a399-4e4e-ad4c-fd3f4ecd2895"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task2.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpic++ -o task2 task2.cpp\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 4 ./task2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7RxuX6u02N2",
        "outputId": "c7b620ce-16be-459b-b62b-7cc143f9a7df"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution vector x:\n",
            "x[0] = 0.714286\n",
            "x[1] = 0.714286\n",
            "x[2] = 0.714286\n",
            "x[3] = 0.714286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 3: Параллельный анализ графов (поиск кратчайших путей)"
      ],
      "metadata": {
        "id": "sLoxCPQI1vvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task3.cpp\n",
        "\n",
        "#include <mpi.h>        // MPI библиотека\n",
        "#include <iostream>    // ввод-вывод\n",
        "#include <vector>      // std::vector\n",
        "#include <algorithm>   // std::min\n",
        "#include <cstdlib>     // rand\n",
        "\n",
        "// Значение, используемое как \"бесконечность\"\n",
        "const double INF = 1e9;\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "\n",
        "    // Инициализация среды MPI\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank, size;\n",
        "\n",
        "    // Получение номера текущего процесса\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "\n",
        "    // Получение общего количества процессов\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    // ===== Параметры задачи =====\n",
        "    // Размер графа (NxN)\n",
        "    int N = 6; // можно изменить или передать через аргументы\n",
        "\n",
        "    // ===== Полная матрица смежности графа =====\n",
        "    // Создаётся и инициализируется только на процессе rank = 0\n",
        "    std::vector<double> G;\n",
        "    if (rank == 0) {\n",
        "        G.resize(N * N);\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            for (int j = 0; j < N; j++) {\n",
        "                if (i == j)\n",
        "                    G[i * N + j] = 0.0;               // расстояние до себя\n",
        "                else\n",
        "                    G[i * N + j] = (rand() % 10) + 1; // случайный вес ребра\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // ===== Распределение строк матрицы между процессами =====\n",
        "    // Базовое количество строк на процесс\n",
        "    int base = N / size;\n",
        "\n",
        "    // Остаток строк, если N не делится на size\n",
        "    int rem  = N % size;\n",
        "\n",
        "    // Количество строк, обрабатываемых текущим процессом\n",
        "    int local_rows = base + (rank < rem ? 1 : 0);\n",
        "\n",
        "    // Смещение глобальных строк для текущего процесса\n",
        "    int offset = rank * base + std::min(rank, rem);\n",
        "\n",
        "    // ===== Локальная часть матрицы смежности =====\n",
        "    std::vector<double> local_G(local_rows * N);\n",
        "\n",
        "    // ===== Параметры для MPI_Scatterv / MPI_Allgatherv / MPI_Gatherv =====\n",
        "    // ВАЖНО: эти массивы должны существовать на ВСЕХ процессах\n",
        "    std::vector<int> sendcounts(size);\n",
        "    std::vector<int> displs(size);\n",
        "\n",
        "    int disp = 0;\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        int rows = base + (i < rem ? 1 : 0);\n",
        "        sendcounts[i] = rows * N; // количество элементов\n",
        "        displs[i] = disp;         // смещение\n",
        "        disp += rows * N;\n",
        "    }\n",
        "\n",
        "    // ===== Распределение строк матрицы между процессами =====\n",
        "    MPI_Scatterv(\n",
        "        G.data(),                 // исходная матрица (rank 0)\n",
        "        sendcounts.data(),        // сколько отправлять\n",
        "        displs.data(),            // смещения\n",
        "        MPI_DOUBLE,\n",
        "        local_G.data(),           // локальный буфер\n",
        "        local_rows * N,           // размер локального буфера\n",
        "        MPI_DOUBLE,\n",
        "        0,\n",
        "        MPI_COMM_WORLD\n",
        "    );\n",
        "\n",
        "    // ===== Буфер для хранения полной матрицы на каждом процессе =====\n",
        "    // Необходим для MPI_Allgatherv\n",
        "    std::vector<double> full_G(N * N);\n",
        "\n",
        "    // ===== Алгоритм Флойда–Уоршелла =====\n",
        "    for (int k = 0; k < N; k++) {\n",
        "\n",
        "        // Все процессы получают актуальную версию матрицы\n",
        "        MPI_Allgatherv(\n",
        "            local_G.data(),\n",
        "            local_rows * N,\n",
        "            MPI_DOUBLE,\n",
        "            full_G.data(),\n",
        "            sendcounts.data(),\n",
        "            displs.data(),\n",
        "            MPI_DOUBLE,\n",
        "            MPI_COMM_WORLD\n",
        "        );\n",
        "\n",
        "        // Обновление расстояний для локальных строк\n",
        "        for (int i = 0; i < local_rows; i++) {\n",
        "            int global_i = offset + i;\n",
        "\n",
        "            for (int j = 0; j < N; j++) {\n",
        "                // Проверка пути через промежуточную вершину k\n",
        "                double new_dist =\n",
        "                    full_G[global_i * N + k] + full_G[k * N + j];\n",
        "\n",
        "                // Если путь короче — обновляем\n",
        "                if (new_dist < local_G[i * N + j]) {\n",
        "                    local_G[i * N + j] = new_dist;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // ===== Сбор итоговой матрицы кратчайших путей =====\n",
        "    MPI_Gatherv(\n",
        "        local_G.data(),\n",
        "        local_rows * N,\n",
        "        MPI_DOUBLE,\n",
        "        G.data(),\n",
        "        sendcounts.data(),\n",
        "        displs.data(),\n",
        "        MPI_DOUBLE,\n",
        "        0,\n",
        "        MPI_COMM_WORLD\n",
        "    );\n",
        "\n",
        "    // ===== Вывод результата =====\n",
        "    if (rank == 0) {\n",
        "        std::cout << \"Shortest paths matrix:\\n\";\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            for (int j = 0; j < N; j++) {\n",
        "                std::cout << G[i * N + j] << \" \";\n",
        "            }\n",
        "            std::cout << std::endl;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Завершение работы MPI\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlFjzO5H1wEN",
        "outputId": "b68cce58-ae5d-49c5-aba3-b12989adcf7e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task3.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpic++ -o task3 task3.cpp\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./task3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0aKMl6R2D0p",
        "outputId": "f7b2ad6d-35e9-4c13-9164-7516434c788e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shortest paths matrix:\n",
            "0 4 7 7 6 4 \n",
            "3 0 6 3 6 2 \n",
            "3 2 0 1 4 4 \n",
            "4 1 7 0 3 3 \n",
            "2 6 7 8 0 3 \n",
            "1 3 4 5 6 0 \n"
          ]
        }
      ]
    }
  ]
}